{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COPY_AlisWiskeras",
      "provenance": [],
      "authorship_tag": "ABX9TyOxlTSeLBnoF6s572SA1sec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhura12gj/BD_STTP_2016/blob/master/Federated%20Learning%20in%20limited%20data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFpN1o5-EvFo",
        "colab_type": "code",
        "outputId": "ce8ff15a-cdd0-40ff-9ae5-1122106122e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"madhuragj\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"b4466304f5f680e98ff19ff50fa11bf1\" # key from the json file\n",
        "!kaggle datasets download -d uciml/breast-cancer-wisconsin-data # api copied from kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "breast-cancer-wisconsin-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjAYXV4zE2Lr",
        "colab_type": "code",
        "outputId": "2f0fa1ca-2147-4df0-80b2-6ada1f8731ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip breast-cancer-wisconsin-data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  breast-cancer-wisconsin-data.zip\n",
            "replace data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK52nZN1E2Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_dxtSp-w0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#resmapling\n",
        "def logic(index):\n",
        "    if index % 3 == 0:\n",
        "       return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTjVOiaJ-ww1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('data.csv', skiprows= lambda x: logic(x) )\n",
        "#del data['Unnamed: 32']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqGEGAdhE2Rj",
        "colab_type": "code",
        "outputId": "8b336e3e-5849-43da-8d7e-879457d4e020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>842302</th>\n",
              "      <th>M</th>\n",
              "      <th>17.99</th>\n",
              "      <th>10.38</th>\n",
              "      <th>122.8</th>\n",
              "      <th>1001</th>\n",
              "      <th>0.1184</th>\n",
              "      <th>0.2776</th>\n",
              "      <th>0.3001</th>\n",
              "      <th>0.1471</th>\n",
              "      <th>0.2419</th>\n",
              "      <th>0.07871</th>\n",
              "      <th>1.095</th>\n",
              "      <th>0.9053</th>\n",
              "      <th>8.589</th>\n",
              "      <th>153.4</th>\n",
              "      <th>0.006399</th>\n",
              "      <th>0.04904</th>\n",
              "      <th>0.05373</th>\n",
              "      <th>0.01587</th>\n",
              "      <th>0.03003</th>\n",
              "      <th>0.006193</th>\n",
              "      <th>25.38</th>\n",
              "      <th>17.33</th>\n",
              "      <th>184.6</th>\n",
              "      <th>2019</th>\n",
              "      <th>0.1622</th>\n",
              "      <th>0.6656</th>\n",
              "      <th>0.7119</th>\n",
              "      <th>0.2654</th>\n",
              "      <th>0.4601</th>\n",
              "      <th>0.1189</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>844359</td>\n",
              "      <td>M</td>\n",
              "      <td>18.25</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.01382</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.01039</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.880</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.14420</td>\n",
              "      <td>0.25760</td>\n",
              "      <td>0.3784</td>\n",
              "      <td>0.1932</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84458202</td>\n",
              "      <td>M</td>\n",
              "      <td>13.71</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.03029</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.01448</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.060</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.16540</td>\n",
              "      <td>0.36820</td>\n",
              "      <td>0.2678</td>\n",
              "      <td>0.1556</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>925622</td>\n",
              "      <td>M</td>\n",
              "      <td>15.22</td>\n",
              "      <td>30.62</td>\n",
              "      <td>103.40</td>\n",
              "      <td>716.9</td>\n",
              "      <td>0.10480</td>\n",
              "      <td>0.20870</td>\n",
              "      <td>0.25500</td>\n",
              "      <td>0.09429</td>\n",
              "      <td>0.2128</td>\n",
              "      <td>0.07152</td>\n",
              "      <td>0.2602</td>\n",
              "      <td>1.2050</td>\n",
              "      <td>2.362</td>\n",
              "      <td>22.65</td>\n",
              "      <td>0.004625</td>\n",
              "      <td>0.04844</td>\n",
              "      <td>0.07359</td>\n",
              "      <td>0.01608</td>\n",
              "      <td>0.02137</td>\n",
              "      <td>0.006142</td>\n",
              "      <td>17.520</td>\n",
              "      <td>42.79</td>\n",
              "      <td>128.70</td>\n",
              "      <td>915.0</td>\n",
              "      <td>0.14170</td>\n",
              "      <td>0.79170</td>\n",
              "      <td>1.1700</td>\n",
              "      <td>0.2356</td>\n",
              "      <td>0.4089</td>\n",
              "      <td>0.14090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>379 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       842302  M  17.99  10.38  ...  0.7119  0.2654  0.4601   0.1189\n",
              "0      842517  M  20.57  17.77  ...  0.2416  0.1860  0.2750  0.08902\n",
              "1    84348301  M  11.42  20.38  ...  0.6869  0.2575  0.6638  0.17300\n",
              "2    84358402  M  20.29  14.34  ...  0.4000  0.1625  0.2364  0.07678\n",
              "3      844359  M  18.25  19.98  ...  0.3784  0.1932  0.3063  0.08368\n",
              "4    84458202  M  13.71  20.83  ...  0.2678  0.1556  0.3196  0.11510\n",
              "..        ... ..    ...    ...  ...     ...     ...     ...      ...\n",
              "374    925622  M  15.22  30.62  ...  1.1700  0.2356  0.4089  0.14090\n",
              "375    926424  M  21.56  22.39  ...  0.4107  0.2216  0.2060  0.07115\n",
              "376    926682  M  20.13  28.25  ...  0.3215  0.1628  0.2572  0.06637\n",
              "377    927241  M  20.60  29.33  ...  0.9387  0.2650  0.4087  0.12400\n",
              "378     92751  B   7.76  24.54  ...  0.0000  0.0000  0.2871  0.07039\n",
              "\n",
              "[379 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "672PgLt7E2Uf",
        "colab_type": "code",
        "outputId": "5a998811-731f-4b52-c4b2-068c1580f727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "#randomizing\n",
        "df1=df.iloc[np.random.permutation(len(df))]\n",
        "df1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>842302</th>\n",
              "      <th>M</th>\n",
              "      <th>17.99</th>\n",
              "      <th>10.38</th>\n",
              "      <th>122.8</th>\n",
              "      <th>1001</th>\n",
              "      <th>0.1184</th>\n",
              "      <th>0.2776</th>\n",
              "      <th>0.3001</th>\n",
              "      <th>0.1471</th>\n",
              "      <th>0.2419</th>\n",
              "      <th>0.07871</th>\n",
              "      <th>1.095</th>\n",
              "      <th>0.9053</th>\n",
              "      <th>8.589</th>\n",
              "      <th>153.4</th>\n",
              "      <th>0.006399</th>\n",
              "      <th>0.04904</th>\n",
              "      <th>0.05373</th>\n",
              "      <th>0.01587</th>\n",
              "      <th>0.03003</th>\n",
              "      <th>0.006193</th>\n",
              "      <th>25.38</th>\n",
              "      <th>17.33</th>\n",
              "      <th>184.6</th>\n",
              "      <th>2019</th>\n",
              "      <th>0.1622</th>\n",
              "      <th>0.6656</th>\n",
              "      <th>0.7119</th>\n",
              "      <th>0.2654</th>\n",
              "      <th>0.4601</th>\n",
              "      <th>0.1189</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>85922302</td>\n",
              "      <td>M</td>\n",
              "      <td>12.680</td>\n",
              "      <td>23.84</td>\n",
              "      <td>82.69</td>\n",
              "      <td>499.0</td>\n",
              "      <td>0.11220</td>\n",
              "      <td>0.12620</td>\n",
              "      <td>0.11280</td>\n",
              "      <td>0.06873</td>\n",
              "      <td>0.1905</td>\n",
              "      <td>0.06590</td>\n",
              "      <td>0.4255</td>\n",
              "      <td>1.1780</td>\n",
              "      <td>2.927</td>\n",
              "      <td>36.460</td>\n",
              "      <td>0.007781</td>\n",
              "      <td>0.026480</td>\n",
              "      <td>0.029730</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.01635</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>17.090</td>\n",
              "      <td>33.47</td>\n",
              "      <td>111.80</td>\n",
              "      <td>888.3</td>\n",
              "      <td>0.1851</td>\n",
              "      <td>0.40610</td>\n",
              "      <td>0.4024</td>\n",
              "      <td>0.17160</td>\n",
              "      <td>0.3383</td>\n",
              "      <td>0.10310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>88350402</td>\n",
              "      <td>B</td>\n",
              "      <td>13.640</td>\n",
              "      <td>15.60</td>\n",
              "      <td>87.38</td>\n",
              "      <td>575.3</td>\n",
              "      <td>0.09423</td>\n",
              "      <td>0.06630</td>\n",
              "      <td>0.04705</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.1717</td>\n",
              "      <td>0.05660</td>\n",
              "      <td>0.3242</td>\n",
              "      <td>0.6612</td>\n",
              "      <td>1.996</td>\n",
              "      <td>27.190</td>\n",
              "      <td>0.006470</td>\n",
              "      <td>0.012480</td>\n",
              "      <td>0.018100</td>\n",
              "      <td>0.011030</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>14.850</td>\n",
              "      <td>19.05</td>\n",
              "      <td>94.11</td>\n",
              "      <td>683.4</td>\n",
              "      <td>0.1278</td>\n",
              "      <td>0.12910</td>\n",
              "      <td>0.1533</td>\n",
              "      <td>0.09222</td>\n",
              "      <td>0.2530</td>\n",
              "      <td>0.06510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>909445</td>\n",
              "      <td>M</td>\n",
              "      <td>17.270</td>\n",
              "      <td>25.42</td>\n",
              "      <td>112.40</td>\n",
              "      <td>928.8</td>\n",
              "      <td>0.08331</td>\n",
              "      <td>0.11090</td>\n",
              "      <td>0.12040</td>\n",
              "      <td>0.05736</td>\n",
              "      <td>0.1467</td>\n",
              "      <td>0.05407</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>1.6790</td>\n",
              "      <td>3.283</td>\n",
              "      <td>58.380</td>\n",
              "      <td>0.008109</td>\n",
              "      <td>0.043080</td>\n",
              "      <td>0.049420</td>\n",
              "      <td>0.017420</td>\n",
              "      <td>0.01594</td>\n",
              "      <td>0.003739</td>\n",
              "      <td>20.380</td>\n",
              "      <td>35.46</td>\n",
              "      <td>132.80</td>\n",
              "      <td>1284.0</td>\n",
              "      <td>0.1436</td>\n",
              "      <td>0.41220</td>\n",
              "      <td>0.5036</td>\n",
              "      <td>0.17390</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.07944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>894329</td>\n",
              "      <td>B</td>\n",
              "      <td>9.042</td>\n",
              "      <td>18.90</td>\n",
              "      <td>60.07</td>\n",
              "      <td>244.5</td>\n",
              "      <td>0.09968</td>\n",
              "      <td>0.19720</td>\n",
              "      <td>0.19750</td>\n",
              "      <td>0.04908</td>\n",
              "      <td>0.2330</td>\n",
              "      <td>0.08743</td>\n",
              "      <td>0.4653</td>\n",
              "      <td>1.9110</td>\n",
              "      <td>3.769</td>\n",
              "      <td>24.200</td>\n",
              "      <td>0.009845</td>\n",
              "      <td>0.065900</td>\n",
              "      <td>0.102700</td>\n",
              "      <td>0.025270</td>\n",
              "      <td>0.03491</td>\n",
              "      <td>0.007877</td>\n",
              "      <td>10.060</td>\n",
              "      <td>23.40</td>\n",
              "      <td>68.62</td>\n",
              "      <td>297.1</td>\n",
              "      <td>0.1221</td>\n",
              "      <td>0.37480</td>\n",
              "      <td>0.4609</td>\n",
              "      <td>0.11450</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>0.10550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>8712291</td>\n",
              "      <td>B</td>\n",
              "      <td>14.970</td>\n",
              "      <td>19.76</td>\n",
              "      <td>95.50</td>\n",
              "      <td>690.2</td>\n",
              "      <td>0.08421</td>\n",
              "      <td>0.05352</td>\n",
              "      <td>0.01947</td>\n",
              "      <td>0.01939</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.05266</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>1.0650</td>\n",
              "      <td>1.286</td>\n",
              "      <td>16.640</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>0.007983</td>\n",
              "      <td>0.008268</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>0.01924</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>15.980</td>\n",
              "      <td>25.82</td>\n",
              "      <td>102.30</td>\n",
              "      <td>782.1</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>0.09995</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.05754</td>\n",
              "      <td>0.2646</td>\n",
              "      <td>0.06085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>897630</td>\n",
              "      <td>M</td>\n",
              "      <td>18.770</td>\n",
              "      <td>21.43</td>\n",
              "      <td>122.90</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>0.09116</td>\n",
              "      <td>0.14020</td>\n",
              "      <td>0.10600</td>\n",
              "      <td>0.06090</td>\n",
              "      <td>0.1953</td>\n",
              "      <td>0.06083</td>\n",
              "      <td>0.6422</td>\n",
              "      <td>1.5300</td>\n",
              "      <td>4.369</td>\n",
              "      <td>88.250</td>\n",
              "      <td>0.007548</td>\n",
              "      <td>0.038970</td>\n",
              "      <td>0.039140</td>\n",
              "      <td>0.018160</td>\n",
              "      <td>0.02168</td>\n",
              "      <td>0.004445</td>\n",
              "      <td>24.540</td>\n",
              "      <td>34.37</td>\n",
              "      <td>161.10</td>\n",
              "      <td>1873.0</td>\n",
              "      <td>0.1498</td>\n",
              "      <td>0.48270</td>\n",
              "      <td>0.4634</td>\n",
              "      <td>0.20480</td>\n",
              "      <td>0.3679</td>\n",
              "      <td>0.09870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>894855</td>\n",
              "      <td>B</td>\n",
              "      <td>12.860</td>\n",
              "      <td>13.32</td>\n",
              "      <td>82.82</td>\n",
              "      <td>504.8</td>\n",
              "      <td>0.11340</td>\n",
              "      <td>0.08834</td>\n",
              "      <td>0.03800</td>\n",
              "      <td>0.03400</td>\n",
              "      <td>0.1543</td>\n",
              "      <td>0.06476</td>\n",
              "      <td>0.2212</td>\n",
              "      <td>1.0420</td>\n",
              "      <td>1.614</td>\n",
              "      <td>16.570</td>\n",
              "      <td>0.005910</td>\n",
              "      <td>0.020160</td>\n",
              "      <td>0.019020</td>\n",
              "      <td>0.010110</td>\n",
              "      <td>0.01202</td>\n",
              "      <td>0.003107</td>\n",
              "      <td>14.040</td>\n",
              "      <td>21.08</td>\n",
              "      <td>92.80</td>\n",
              "      <td>599.5</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>0.22310</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.11550</td>\n",
              "      <td>0.2382</td>\n",
              "      <td>0.08553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>863030</td>\n",
              "      <td>M</td>\n",
              "      <td>13.110</td>\n",
              "      <td>15.56</td>\n",
              "      <td>87.21</td>\n",
              "      <td>530.2</td>\n",
              "      <td>0.13980</td>\n",
              "      <td>0.17650</td>\n",
              "      <td>0.20710</td>\n",
              "      <td>0.09601</td>\n",
              "      <td>0.1925</td>\n",
              "      <td>0.07692</td>\n",
              "      <td>0.3908</td>\n",
              "      <td>0.9238</td>\n",
              "      <td>2.410</td>\n",
              "      <td>34.660</td>\n",
              "      <td>0.007162</td>\n",
              "      <td>0.029120</td>\n",
              "      <td>0.054730</td>\n",
              "      <td>0.013880</td>\n",
              "      <td>0.01547</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>16.310</td>\n",
              "      <td>22.40</td>\n",
              "      <td>106.40</td>\n",
              "      <td>827.2</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.40990</td>\n",
              "      <td>0.6376</td>\n",
              "      <td>0.19860</td>\n",
              "      <td>0.3147</td>\n",
              "      <td>0.14050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>899987</td>\n",
              "      <td>M</td>\n",
              "      <td>25.730</td>\n",
              "      <td>17.46</td>\n",
              "      <td>174.20</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>0.11490</td>\n",
              "      <td>0.23630</td>\n",
              "      <td>0.33680</td>\n",
              "      <td>0.19130</td>\n",
              "      <td>0.1956</td>\n",
              "      <td>0.06121</td>\n",
              "      <td>0.9948</td>\n",
              "      <td>0.8509</td>\n",
              "      <td>7.222</td>\n",
              "      <td>153.100</td>\n",
              "      <td>0.006369</td>\n",
              "      <td>0.042430</td>\n",
              "      <td>0.042660</td>\n",
              "      <td>0.015080</td>\n",
              "      <td>0.02335</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>33.130</td>\n",
              "      <td>23.58</td>\n",
              "      <td>229.30</td>\n",
              "      <td>3234.0</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.59370</td>\n",
              "      <td>0.6451</td>\n",
              "      <td>0.27560</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.08815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>864496</td>\n",
              "      <td>B</td>\n",
              "      <td>8.726</td>\n",
              "      <td>15.83</td>\n",
              "      <td>55.84</td>\n",
              "      <td>230.9</td>\n",
              "      <td>0.11500</td>\n",
              "      <td>0.08201</td>\n",
              "      <td>0.04132</td>\n",
              "      <td>0.01924</td>\n",
              "      <td>0.1649</td>\n",
              "      <td>0.07633</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.5864</td>\n",
              "      <td>1.354</td>\n",
              "      <td>8.966</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>0.022130</td>\n",
              "      <td>0.032590</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.01708</td>\n",
              "      <td>0.003806</td>\n",
              "      <td>9.628</td>\n",
              "      <td>19.62</td>\n",
              "      <td>64.48</td>\n",
              "      <td>284.4</td>\n",
              "      <td>0.1724</td>\n",
              "      <td>0.23640</td>\n",
              "      <td>0.2456</td>\n",
              "      <td>0.10500</td>\n",
              "      <td>0.2926</td>\n",
              "      <td>0.10170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>379 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       842302  M   17.99  10.38  ...  0.7119   0.2654  0.4601   0.1189\n",
              "42   85922302  M  12.680  23.84  ...  0.4024  0.17160  0.3383  0.10310\n",
              "159  88350402  B  13.640  15.60  ...  0.1533  0.09222  0.2530  0.06510\n",
              "293    909445  M  17.270  25.42  ...  0.5036  0.17390  0.2500  0.07944\n",
              "211    894329  B   9.042  18.90  ...  0.4609  0.11450  0.3135  0.10550\n",
              "109   8712291  B  14.970  19.76  ...  0.0775  0.05754  0.2646  0.06085\n",
              "..        ... ..     ...    ...  ...     ...      ...     ...      ...\n",
              "224    897630  M  18.770  21.43  ...  0.4634  0.20480  0.3679  0.09870\n",
              "214    894855  B  12.860  13.32  ...  0.1791  0.11550  0.2382  0.08553\n",
              "69     863030  M  13.110  15.56  ...  0.6376  0.19860  0.3147  0.14050\n",
              "234    899987  M  25.730  17.46  ...  0.6451  0.27560  0.3690  0.08815\n",
              "75     864496  B   8.726  15.83  ...  0.2456  0.10500  0.2926  0.10170\n",
              "\n",
              "[379 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiJjLALE2XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df1.iloc[:, 2:].values\n",
        "y = df1.iloc[:, 1].values\n",
        "\n",
        "# Encoding categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "y = labelencoder_X_1.fit_transform(y)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091gGILYFCK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Atrain = X_train[0:126]\n",
        "Y_Atrain = y_train[0:126]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM8Q41_EFCOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Btrain = X_train[126:253]\n",
        "Y_Btrain = y_train[126:253]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB5xx9cNFCQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Ctrain = X_train[253:]\n",
        "Y_Ctrain = y_train[253:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6_c3RXJmdv",
        "colab_type": "text"
      },
      "source": [
        "**Alice**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va72u2C0FCTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Atrain = sc.fit_transform(X_Atrain)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkD7Xi0FQrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the ANN\n",
        "classifierA = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AayPSVyFQux",
        "colab_type": "code",
        "outputId": "9c455f21-0b22-484e-e77b-44022d0278e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifierA.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierA.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z1gauN4FQxZ",
        "colab_type": "code",
        "outputId": "148e9ae4-181a-47d0-8633-40d1e7e88229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the second hidden layer\n",
        "classifierA.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierA.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG_jS5SrFQ0a",
        "colab_type": "code",
        "outputId": "3891aac5-14c4-468f-b59c-506a697bb5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Adding the output layer\n",
        "classifierA.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSjpynk7FQ2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the ANN\n",
        "classifierA.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt88yv3cFQ50",
        "colab_type": "code",
        "outputId": "8993ad24-6cfa-4aa6-ef86-ff6c6ee1dc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting the ANN to the Training set\n",
        "classifierA.fit(X_Atrain, Y_Atrain, batch_size=100, nb_epoch=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "126/126 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.4286\n",
            "Epoch 2/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.6928 - accuracy: 0.6270\n",
            "Epoch 3/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.6923 - accuracy: 0.6429\n",
            "Epoch 4/150\n",
            "126/126 [==============================] - 0s 40us/step - loss: 0.6919 - accuracy: 0.6270\n",
            "Epoch 5/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.6913 - accuracy: 0.6429\n",
            "Epoch 6/150\n",
            "126/126 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.6508\n",
            "Epoch 7/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.6900 - accuracy: 0.6667\n",
            "Epoch 8/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.6890 - accuracy: 0.6667\n",
            "Epoch 9/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.6879 - accuracy: 0.6984\n",
            "Epoch 10/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.6864 - accuracy: 0.7460\n",
            "Epoch 11/150\n",
            "126/126 [==============================] - 0s 53us/step - loss: 0.6849 - accuracy: 0.7857\n",
            "Epoch 12/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.6825 - accuracy: 0.8016\n",
            "Epoch 13/150\n",
            "126/126 [==============================] - 0s 41us/step - loss: 0.6804 - accuracy: 0.8254\n",
            "Epoch 14/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.6775 - accuracy: 0.8651\n",
            "Epoch 15/150\n",
            "126/126 [==============================] - 0s 35us/step - loss: 0.6745 - accuracy: 0.8810\n",
            "Epoch 16/150\n",
            "126/126 [==============================] - 0s 35us/step - loss: 0.6713 - accuracy: 0.8810\n",
            "Epoch 17/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.6658 - accuracy: 0.8968\n",
            "Epoch 18/150\n",
            "126/126 [==============================] - 0s 40us/step - loss: 0.6606 - accuracy: 0.8968\n",
            "Epoch 19/150\n",
            "126/126 [==============================] - 0s 63us/step - loss: 0.6544 - accuracy: 0.9048\n",
            "Epoch 20/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.6485 - accuracy: 0.9127\n",
            "Epoch 21/150\n",
            "126/126 [==============================] - 0s 57us/step - loss: 0.6411 - accuracy: 0.9127\n",
            "Epoch 22/150\n",
            "126/126 [==============================] - 0s 41us/step - loss: 0.6310 - accuracy: 0.9206\n",
            "Epoch 23/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.6205 - accuracy: 0.9286\n",
            "Epoch 24/150\n",
            "126/126 [==============================] - 0s 37us/step - loss: 0.6098 - accuracy: 0.9365\n",
            "Epoch 25/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.5982 - accuracy: 0.9286\n",
            "Epoch 26/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.5840 - accuracy: 0.9286\n",
            "Epoch 27/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.5724 - accuracy: 0.9206\n",
            "Epoch 28/150\n",
            "126/126 [==============================] - 0s 41us/step - loss: 0.5544 - accuracy: 0.9206\n",
            "Epoch 29/150\n",
            "126/126 [==============================] - 0s 94us/step - loss: 0.5396 - accuracy: 0.9286\n",
            "Epoch 30/150\n",
            "126/126 [==============================] - 0s 54us/step - loss: 0.5251 - accuracy: 0.9286\n",
            "Epoch 31/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.5060 - accuracy: 0.9206\n",
            "Epoch 32/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.4861 - accuracy: 0.9206\n",
            "Epoch 33/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.4727 - accuracy: 0.9206\n",
            "Epoch 34/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.4514 - accuracy: 0.9286\n",
            "Epoch 35/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.4253 - accuracy: 0.9206\n",
            "Epoch 36/150\n",
            "126/126 [==============================] - 0s 43us/step - loss: 0.4169 - accuracy: 0.9286\n",
            "Epoch 37/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.3895 - accuracy: 0.9206\n",
            "Epoch 38/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.3756 - accuracy: 0.9286\n",
            "Epoch 39/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.3532 - accuracy: 0.9286\n",
            "Epoch 40/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.3348 - accuracy: 0.9286\n",
            "Epoch 41/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.3185 - accuracy: 0.9286\n",
            "Epoch 42/150\n",
            "126/126 [==============================] - 0s 63us/step - loss: 0.3042 - accuracy: 0.9286\n",
            "Epoch 43/150\n",
            "126/126 [==============================] - 0s 57us/step - loss: 0.2941 - accuracy: 0.9286\n",
            "Epoch 44/150\n",
            "126/126 [==============================] - 0s 70us/step - loss: 0.2747 - accuracy: 0.9365\n",
            "Epoch 45/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.2705 - accuracy: 0.9365\n",
            "Epoch 46/150\n",
            "126/126 [==============================] - 0s 59us/step - loss: 0.2477 - accuracy: 0.9286\n",
            "Epoch 47/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.2332 - accuracy: 0.9286\n",
            "Epoch 48/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.2245 - accuracy: 0.9286\n",
            "Epoch 49/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.2087 - accuracy: 0.9286\n",
            "Epoch 50/150\n",
            "126/126 [==============================] - 0s 56us/step - loss: 0.2094 - accuracy: 0.9444\n",
            "Epoch 51/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.1963 - accuracy: 0.9286\n",
            "Epoch 52/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.1779 - accuracy: 0.9286\n",
            "Epoch 53/150\n",
            "126/126 [==============================] - 0s 53us/step - loss: 0.1751 - accuracy: 0.9365\n",
            "Epoch 54/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.1765 - accuracy: 0.9365\n",
            "Epoch 55/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.1725 - accuracy: 0.9444\n",
            "Epoch 56/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.1571 - accuracy: 0.9444\n",
            "Epoch 57/150\n",
            "126/126 [==============================] - 0s 56us/step - loss: 0.1501 - accuracy: 0.9444\n",
            "Epoch 58/150\n",
            "126/126 [==============================] - 0s 45us/step - loss: 0.1510 - accuracy: 0.9365\n",
            "Epoch 59/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.1548 - accuracy: 0.9444\n",
            "Epoch 60/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.1501 - accuracy: 0.9444\n",
            "Epoch 61/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.1460 - accuracy: 0.9444\n",
            "Epoch 62/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.1459 - accuracy: 0.9524\n",
            "Epoch 63/150\n",
            "126/126 [==============================] - 0s 45us/step - loss: 0.1392 - accuracy: 0.9524\n",
            "Epoch 64/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.1376 - accuracy: 0.9444\n",
            "Epoch 65/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.1336 - accuracy: 0.9524\n",
            "Epoch 66/150\n",
            "126/126 [==============================] - 0s 56us/step - loss: 0.1225 - accuracy: 0.9603\n",
            "Epoch 67/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.1213 - accuracy: 0.9524\n",
            "Epoch 68/150\n",
            "126/126 [==============================] - 0s 54us/step - loss: 0.1218 - accuracy: 0.9524\n",
            "Epoch 69/150\n",
            "126/126 [==============================] - 0s 57us/step - loss: 0.1232 - accuracy: 0.9524\n",
            "Epoch 70/150\n",
            "126/126 [==============================] - 0s 60us/step - loss: 0.1243 - accuracy: 0.9524\n",
            "Epoch 71/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.1166 - accuracy: 0.9524\n",
            "Epoch 72/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.1193 - accuracy: 0.9603\n",
            "Epoch 73/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.1108 - accuracy: 0.9603\n",
            "Epoch 74/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.1099 - accuracy: 0.9603\n",
            "Epoch 75/150\n",
            "126/126 [==============================] - 0s 57us/step - loss: 0.1037 - accuracy: 0.9603\n",
            "Epoch 76/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.1105 - accuracy: 0.9603\n",
            "Epoch 77/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.1067 - accuracy: 0.9603\n",
            "Epoch 78/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.1099 - accuracy: 0.9683\n",
            "Epoch 79/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.1072 - accuracy: 0.9683\n",
            "Epoch 80/150\n",
            "126/126 [==============================] - 0s 42us/step - loss: 0.1062 - accuracy: 0.9683\n",
            "Epoch 81/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0987 - accuracy: 0.9683\n",
            "Epoch 82/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.0985 - accuracy: 0.9683\n",
            "Epoch 83/150\n",
            "126/126 [==============================] - 0s 53us/step - loss: 0.0982 - accuracy: 0.9683\n",
            "Epoch 84/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0981 - accuracy: 0.9683\n",
            "Epoch 85/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0985 - accuracy: 0.9683\n",
            "Epoch 86/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.0915 - accuracy: 0.9683\n",
            "Epoch 87/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0964 - accuracy: 0.9683\n",
            "Epoch 88/150\n",
            "126/126 [==============================] - 0s 54us/step - loss: 0.0923 - accuracy: 0.9683\n",
            "Epoch 89/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.0930 - accuracy: 0.9683\n",
            "Epoch 90/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0911 - accuracy: 0.9683\n",
            "Epoch 91/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0939 - accuracy: 0.9683\n",
            "Epoch 92/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.0942 - accuracy: 0.9683\n",
            "Epoch 93/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.0922 - accuracy: 0.9683\n",
            "Epoch 94/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.0864 - accuracy: 0.9683\n",
            "Epoch 95/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.0887 - accuracy: 0.9683\n",
            "Epoch 96/150\n",
            "126/126 [==============================] - 0s 43us/step - loss: 0.0917 - accuracy: 0.9683\n",
            "Epoch 97/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0853 - accuracy: 0.9683\n",
            "Epoch 98/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0890 - accuracy: 0.9683\n",
            "Epoch 99/150\n",
            "126/126 [==============================] - 0s 45us/step - loss: 0.0869 - accuracy: 0.9683\n",
            "Epoch 100/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0859 - accuracy: 0.9683\n",
            "Epoch 101/150\n",
            "126/126 [==============================] - 0s 54us/step - loss: 0.0774 - accuracy: 0.9683\n",
            "Epoch 102/150\n",
            "126/126 [==============================] - 0s 60us/step - loss: 0.0834 - accuracy: 0.9683\n",
            "Epoch 103/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.0804 - accuracy: 0.9683\n",
            "Epoch 104/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0837 - accuracy: 0.9683\n",
            "Epoch 105/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0818 - accuracy: 0.9683\n",
            "Epoch 106/150\n",
            "126/126 [==============================] - 0s 45us/step - loss: 0.0822 - accuracy: 0.9683\n",
            "Epoch 107/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0759 - accuracy: 0.9683\n",
            "Epoch 108/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.0774 - accuracy: 0.9683\n",
            "Epoch 109/150\n",
            "126/126 [==============================] - 0s 52us/step - loss: 0.0828 - accuracy: 0.9683\n",
            "Epoch 110/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0813 - accuracy: 0.9683\n",
            "Epoch 111/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.0786 - accuracy: 0.9683\n",
            "Epoch 112/150\n",
            "126/126 [==============================] - 0s 59us/step - loss: 0.0805 - accuracy: 0.9683\n",
            "Epoch 113/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.0698 - accuracy: 0.9683\n",
            "Epoch 114/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.0770 - accuracy: 0.9683\n",
            "Epoch 115/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.0755 - accuracy: 0.9683\n",
            "Epoch 116/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.0758 - accuracy: 0.9683\n",
            "Epoch 117/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.0667 - accuracy: 0.9683\n",
            "Epoch 118/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.0798 - accuracy: 0.9683\n",
            "Epoch 119/150\n",
            "126/126 [==============================] - 0s 60us/step - loss: 0.0745 - accuracy: 0.9683\n",
            "Epoch 120/150\n",
            "126/126 [==============================] - 0s 90us/step - loss: 0.0748 - accuracy: 0.9683\n",
            "Epoch 121/150\n",
            "126/126 [==============================] - 0s 68us/step - loss: 0.0749 - accuracy: 0.9683\n",
            "Epoch 122/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0677 - accuracy: 0.9683\n",
            "Epoch 123/150\n",
            "126/126 [==============================] - 0s 58us/step - loss: 0.0709 - accuracy: 0.9683\n",
            "Epoch 124/150\n",
            "126/126 [==============================] - 0s 54us/step - loss: 0.0714 - accuracy: 0.9683\n",
            "Epoch 125/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0710 - accuracy: 0.9683\n",
            "Epoch 126/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0736 - accuracy: 0.9683\n",
            "Epoch 127/150\n",
            "126/126 [==============================] - 0s 50us/step - loss: 0.0781 - accuracy: 0.9683\n",
            "Epoch 128/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0663 - accuracy: 0.9683\n",
            "Epoch 129/150\n",
            "126/126 [==============================] - 0s 57us/step - loss: 0.0686 - accuracy: 0.9683\n",
            "Epoch 130/150\n",
            "126/126 [==============================] - 0s 61us/step - loss: 0.0707 - accuracy: 0.9683\n",
            "Epoch 131/150\n",
            "126/126 [==============================] - 0s 53us/step - loss: 0.0695 - accuracy: 0.9683\n",
            "Epoch 132/150\n",
            "126/126 [==============================] - 0s 59us/step - loss: 0.0670 - accuracy: 0.9683\n",
            "Epoch 133/150\n",
            "126/126 [==============================] - 0s 41us/step - loss: 0.0642 - accuracy: 0.9683\n",
            "Epoch 134/150\n",
            "126/126 [==============================] - 0s 74us/step - loss: 0.0725 - accuracy: 0.9683\n",
            "Epoch 135/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.0635 - accuracy: 0.9683\n",
            "Epoch 136/150\n",
            "126/126 [==============================] - 0s 53us/step - loss: 0.0616 - accuracy: 0.9762\n",
            "Epoch 137/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0690 - accuracy: 0.9683\n",
            "Epoch 138/150\n",
            "126/126 [==============================] - 0s 48us/step - loss: 0.0629 - accuracy: 0.9683\n",
            "Epoch 139/150\n",
            "126/126 [==============================] - 0s 95us/step - loss: 0.0685 - accuracy: 0.9683\n",
            "Epoch 140/150\n",
            "126/126 [==============================] - 0s 43us/step - loss: 0.0665 - accuracy: 0.9683\n",
            "Epoch 141/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.0623 - accuracy: 0.9762\n",
            "Epoch 142/150\n",
            "126/126 [==============================] - 0s 55us/step - loss: 0.0606 - accuracy: 0.9683\n",
            "Epoch 143/150\n",
            "126/126 [==============================] - 0s 51us/step - loss: 0.0671 - accuracy: 0.9683\n",
            "Epoch 144/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0603 - accuracy: 0.9762\n",
            "Epoch 145/150\n",
            "126/126 [==============================] - 0s 46us/step - loss: 0.0641 - accuracy: 0.9683\n",
            "Epoch 146/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0622 - accuracy: 0.9762\n",
            "Epoch 147/150\n",
            "126/126 [==============================] - 0s 43us/step - loss: 0.0590 - accuracy: 0.9683\n",
            "Epoch 148/150\n",
            "126/126 [==============================] - 0s 49us/step - loss: 0.0607 - accuracy: 0.9683\n",
            "Epoch 149/150\n",
            "126/126 [==============================] - 0s 44us/step - loss: 0.0631 - accuracy: 0.9683\n",
            "Epoch 150/150\n",
            "126/126 [==============================] - 0s 47us/step - loss: 0.0574 - accuracy: 0.9762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f399e5f5cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYLs_PEmFQ8v",
        "colab_type": "code",
        "outputId": "5340e9a5-0487-4561-8d77-df03138be1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_Apred = classifierA.predict(X_test)\n",
        "y_Apred = (y_Apred > 0.5)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(y_Apred))*100\n",
        "cm = confusion_matrix(y_test, np.round(y_Apred))\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX')\n",
        "print(cm)\n",
        "\n",
        "print('\\nTEST METRICS')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX\n",
            "[[50  3]\n",
            " [ 2 21]]\n",
            "\n",
            "TEST METRICS\n",
            "Accuracy: 93.42105263157895%\n",
            "Precision: 87.5%\n",
            "Recall: 91.30434782608695%\n",
            "F1-score: 89.36170212765957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097crTPgFQ_W",
        "colab_type": "code",
        "outputId": "dc4ba520-c019-4b92-b099-e7302c48fec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifierA.get_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.15886317,  0.21452233, -0.13780814,  0.21021369, -0.196352  ,\n",
              "          0.2111986 , -0.14198941, -0.1807868 , -0.12493242, -0.20784943,\n",
              "         -0.1452234 ,  0.2246286 , -0.13705455,  0.13014665,  0.22160284,\n",
              "         -0.15719107],\n",
              "        [ 0.18712543,  0.24821082, -0.20843995,  0.20007305, -0.21775687,\n",
              "          0.14126222, -0.1612427 , -0.18716441, -0.21322614, -0.224075  ,\n",
              "         -0.24341081,  0.14422697, -0.19598518,  0.1538494 ,  0.23693131,\n",
              "         -0.19759311],\n",
              "        [ 0.21099931,  0.22278616, -0.13797042,  0.21113405, -0.17379616,\n",
              "          0.14732406, -0.19710426, -0.19274694, -0.17180127, -0.15376195,\n",
              "         -0.14087985,  0.19818603, -0.13410994,  0.21349554,  0.15322295,\n",
              "         -0.16170412],\n",
              "        [ 0.22447643,  0.19116388, -0.14720444,  0.1768887 , -0.20526762,\n",
              "          0.15216547, -0.19198158, -0.1971588 , -0.19115701, -0.18660359,\n",
              "         -0.15887645,  0.20595731, -0.21437892,  0.19612691,  0.21396315,\n",
              "         -0.1412576 ],\n",
              "        [ 0.07951741,  0.06230343, -0.03380946,  0.1408881 ,  0.07449184,\n",
              "          0.06014764,  0.10170168,  0.13350129,  0.12561353,  0.03448782,\n",
              "         -0.01874832,  0.07926147,  0.10738032,  0.12595975,  0.11098379,\n",
              "         -0.04504041],\n",
              "        [-0.02171841,  0.06182349, -0.10699905,  0.07143982, -0.14042336,\n",
              "          0.09140948, -0.11786927, -0.13629326, -0.13854681, -0.05601492,\n",
              "         -0.15627253,  0.1187518 , -0.10049872,  0.13737187,  0.10812097,\n",
              "         -0.15277377],\n",
              "        [ 0.13867126,  0.16204542, -0.21346366,  0.13864756, -0.21552633,\n",
              "          0.20956469, -0.1520442 , -0.13064785, -0.1540225 , -0.15462759,\n",
              "         -0.21403997,  0.1254679 , -0.13914517,  0.14940596,  0.18043363,\n",
              "         -0.18622933],\n",
              "        [ 0.21677688,  0.21638142, -0.21654634,  0.23728158, -0.15312742,\n",
              "          0.22865884, -0.19622394, -0.19725654, -0.17000185, -0.16410586,\n",
              "         -0.21988155,  0.22967093, -0.20157595,  0.2137455 ,  0.16970299,\n",
              "         -0.18018402],\n",
              "        [-0.06714264, -0.02184431, -0.07478093, -0.01145982,  0.01975454,\n",
              "         -0.02203121, -0.043791  , -0.05598067, -0.00583486, -0.05227414,\n",
              "         -0.00580274, -0.04840101, -0.04139093, -0.04740828,  0.03054147,\n",
              "         -0.09323982],\n",
              "        [-0.18149556, -0.18313085,  0.11029906, -0.10861587,  0.10177749,\n",
              "         -0.15921493,  0.1762193 ,  0.1547751 ,  0.15226541,  0.20639919,\n",
              "          0.1582939 , -0.14742099,  0.1460811 , -0.11378682, -0.09062208,\n",
              "          0.20218349],\n",
              "        [ 0.224422  ,  0.19734156, -0.18864591,  0.22877038, -0.23521788,\n",
              "          0.26234955, -0.23824969, -0.17345656, -0.20890212, -0.16163743,\n",
              "         -0.17652875,  0.18695112, -0.18392867,  0.17469944,  0.21509445,\n",
              "         -0.20045704],\n",
              "        [ 0.09673245,  0.05922555, -0.04416231, -0.00918738, -0.06313002,\n",
              "         -0.00392883,  0.0297815 ,  0.13903013,  0.10021779,  0.10020993,\n",
              "         -0.05626198,  0.01423663,  0.09391136, -0.0044828 ,  0.1069429 ,\n",
              "          0.07766156],\n",
              "        [ 0.15930313,  0.08029385, -0.17311764,  0.08726551, -0.21250556,\n",
              "          0.08159287, -0.14885344, -0.14034289, -0.14479892, -0.14039569,\n",
              "         -0.18319426,  0.08761267, -0.1334641 ,  0.08092172,  0.11378617,\n",
              "         -0.1860137 ],\n",
              "        [ 0.19918233,  0.17101648, -0.20459801,  0.1816135 , -0.16455224,\n",
              "          0.17975819, -0.1659812 , -0.2159032 , -0.19381799, -0.2135516 ,\n",
              "         -0.17854278,  0.18118927, -0.26788703,  0.21257643,  0.19709636,\n",
              "         -0.15749934],\n",
              "        [-0.02337674, -0.08393638,  0.04020444, -0.12816583,  0.15711094,\n",
              "         -0.10283867,  0.20133649,  0.22325218,  0.24864388,  0.25238568,\n",
              "          0.06728055, -0.06885827,  0.22601312, -0.11128318, -0.03249978,\n",
              "          0.12415998],\n",
              "        [-0.1607518 , -0.03788132, -0.09900696, -0.10056494, -0.04584128,\n",
              "         -0.07850129, -0.06984683, -0.03346851, -0.02831057,  0.00522374,\n",
              "         -0.05651091, -0.03008417,  0.01779929, -0.094078  , -0.01407261,\n",
              "          0.0016928 ],\n",
              "        [ 0.03589776,  0.01820909, -0.01230342, -0.01102121,  0.03901842,\n",
              "         -0.07140712, -0.00680808,  0.03082755, -0.01093847,  0.05017551,\n",
              "         -0.02937127, -0.06069436,  0.06428535, -0.05260121, -0.0068243 ,\n",
              "          0.06162214],\n",
              "        [ 0.12258796,  0.12418251, -0.05359339,  0.03536688, -0.02636853,\n",
              "          0.09949823, -0.05350762,  0.02325339, -0.02365514,  0.04202139,\n",
              "         -0.02496185,  0.05507332,  0.07349282,  0.03568235,  0.14068742,\n",
              "         -0.02183219],\n",
              "        [-0.0077094 , -0.01350316,  0.16011763, -0.10609188,  0.15865271,\n",
              "         -0.14884734,  0.21790086,  0.16520916,  0.19534041,  0.22858004,\n",
              "          0.16291907, -0.13067892,  0.11928075, -0.13787611, -0.06438264,\n",
              "          0.18784449],\n",
              "        [-0.19278425, -0.12148032, -0.02290645, -0.06292618, -0.07275745,\n",
              "         -0.12723035,  0.01304385,  0.00988215,  0.03991049,  0.055753  ,\n",
              "         -0.04854843, -0.16065656,  0.17764108, -0.13820446, -0.08323278,\n",
              "          0.1421391 ],\n",
              "        [ 0.17174481,  0.15686466, -0.22589543,  0.19823489, -0.21531302,\n",
              "          0.18832932, -0.24696524, -0.17482501, -0.19092828, -0.20927301,\n",
              "         -0.22973968,  0.18498698, -0.28134978,  0.19725665,  0.16314794,\n",
              "         -0.18524843],\n",
              "        [ 0.14819644,  0.24617809, -0.22217818,  0.22011591, -0.24622385,\n",
              "          0.2343454 , -0.17834522, -0.24373986, -0.18846226, -0.1722025 ,\n",
              "         -0.24549264,  0.27066967, -0.22881016,  0.23754932,  0.23635691,\n",
              "         -0.24027617],\n",
              "        [ 0.13673134,  0.21104607, -0.21561345,  0.17973714, -0.16415523,\n",
              "          0.16085032, -0.17116855, -0.19643955, -0.24735844, -0.21270397,\n",
              "         -0.16492696,  0.1872487 , -0.21091552,  0.17275919,  0.14559725,\n",
              "         -0.23189433],\n",
              "        [ 0.22161566,  0.18026762, -0.22611356,  0.14737704, -0.20248096,\n",
              "          0.221638  , -0.26327   , -0.18138273, -0.24879026, -0.20067482,\n",
              "         -0.21024148,  0.16177864, -0.2647305 ,  0.22420509,  0.21831113,\n",
              "         -0.20212132],\n",
              "        [ 0.17606674,  0.18297425, -0.13372816,  0.13624613, -0.01637935,\n",
              "          0.18169977, -0.0277713 , -0.0133072 , -0.00092492, -0.06752875,\n",
              "         -0.07728895,  0.15463181, -0.08167276,  0.18695551,  0.20001614,\n",
              "         -0.15047143],\n",
              "        [ 0.05686333,  0.02661883, -0.18472189,  0.10293362, -0.22021392,\n",
              "          0.0933388 , -0.20771134, -0.22534846, -0.22378376, -0.2142612 ,\n",
              "         -0.22244489,  0.09089021, -0.16892366,  0.07212229,  0.10484263,\n",
              "         -0.21991868],\n",
              "        [ 0.14788271,  0.12788418, -0.19670297,  0.12360696, -0.17105941,\n",
              "          0.08568548, -0.19537139, -0.17707707, -0.16184515, -0.20013523,\n",
              "         -0.13362712,  0.09859717, -0.1167035 ,  0.1285019 ,  0.10900903,\n",
              "         -0.18402089],\n",
              "        [ 0.2391359 ,  0.1575965 , -0.21304022,  0.21999729, -0.16126506,\n",
              "          0.21859221, -0.21185008, -0.18780206, -0.20178932, -0.24752173,\n",
              "         -0.2202506 ,  0.20192873, -0.2307845 ,  0.18702252,  0.20717296,\n",
              "         -0.23614344],\n",
              "        [ 0.2107256 ,  0.18607175, -0.09321952,  0.08278129, -0.07352913,\n",
              "          0.09652032, -0.13626929, -0.12369159, -0.0697879 , -0.11364181,\n",
              "         -0.15297145,  0.07091218, -0.06100313,  0.06250647,  0.20791836,\n",
              "         -0.13388664],\n",
              "        [-0.07491443,  0.04269435, -0.22891383,  0.0337442 , -0.18694109,\n",
              "          0.07286117, -0.17886567, -0.16782822, -0.15904924, -0.14638798,\n",
              "         -0.21859767,  0.13618198, -0.14214806,  0.12819153,  0.00734326,\n",
              "         -0.18155196]], dtype=float32),\n",
              " array([0.11720333, 0.15970843, 0.22115858, 0.16303097, 0.22660893,\n",
              "        0.15380144, 0.1846895 , 0.17948407, 0.17097393, 0.1834076 ,\n",
              "        0.21960121, 0.12598628, 0.19917123, 0.13113335, 0.17169765,\n",
              "        0.18908006], dtype=float32),\n",
              " array([[-1.63957313e-01,  2.05258608e-01, -1.91355333e-01,\n",
              "          2.32014731e-01,  2.92154253e-01, -2.00066417e-01,\n",
              "          2.28548810e-01,  1.58351138e-01,  1.79026902e-01,\n",
              "         -2.13145345e-01,  2.07874149e-01, -1.73386499e-01,\n",
              "         -1.92041367e-01,  2.47660816e-01,  1.83855653e-01,\n",
              "         -2.03592226e-01],\n",
              "        [ 1.92294475e-02,  2.00003102e-01,  1.48573285e-02,\n",
              "          1.18202709e-01,  1.69786364e-01,  4.72056642e-02,\n",
              "          1.65664718e-01,  1.89165056e-01,  1.28728956e-01,\n",
              "          3.48682106e-02,  1.98802382e-01,  4.53873947e-02,\n",
              "          3.07882205e-03,  1.67982489e-01,  1.98411778e-01,\n",
              "          3.61013487e-02],\n",
              "        [ 1.77319765e-01,  8.76988620e-02,  1.45040199e-01,\n",
              "          7.36369267e-02,  8.16235244e-02,  2.10893735e-01,\n",
              "         -2.84280686e-04,  9.84880850e-02,  7.65289143e-02,\n",
              "          2.04114527e-01,  8.49798322e-02,  1.99289948e-01,\n",
              "          1.33010209e-01,  8.32205638e-02,  8.00247788e-02,\n",
              "          1.67117417e-01],\n",
              "        [-3.29230055e-02,  1.81429878e-01, -2.13986970e-02,\n",
              "          2.12843075e-01,  1.57976896e-01, -2.74658240e-02,\n",
              "          1.70296744e-01,  2.03836530e-01,  2.12341398e-01,\n",
              "          1.76127199e-02,  1.93336889e-01, -2.72452869e-02,\n",
              "          5.24836965e-03,  1.56797290e-01,  1.31414533e-01,\n",
              "         -2.19779182e-03],\n",
              "        [ 2.04535529e-01, -4.55959029e-02,  1.42764449e-01,\n",
              "         -1.93756800e-02, -3.24039347e-02,  1.49440393e-01,\n",
              "         -8.82793069e-02,  9.64723248e-03, -3.05627361e-02,\n",
              "          2.38805652e-01, -1.50926262e-02,  1.91376507e-01,\n",
              "          1.91580340e-01, -4.59199026e-02, -4.42080908e-02,\n",
              "          2.23468855e-01],\n",
              "        [-7.62926415e-02,  1.49746761e-01, -6.54026344e-02,\n",
              "          2.04705566e-01,  1.68946072e-01, -4.91310284e-02,\n",
              "          2.25103617e-01,  1.53188109e-01,  2.20606357e-01,\n",
              "         -5.25504425e-02,  1.81628793e-01, -6.22956865e-02,\n",
              "         -4.13490348e-02,  2.12716803e-01,  1.42848670e-01,\n",
              "         -3.56829613e-02],\n",
              "        [ 2.11678237e-01, -5.90897538e-03,  1.43467695e-01,\n",
              "         -8.06604791e-03, -2.28764154e-02,  1.70246139e-01,\n",
              "         -7.06246588e-03, -2.17805840e-02, -7.55028194e-03,\n",
              "          2.16017023e-01, -1.05383375e-03,  1.93134576e-01,\n",
              "          2.29741663e-01, -4.75122966e-03, -2.09090933e-02,\n",
              "          1.82269976e-01],\n",
              "        [ 2.55191237e-01, -6.23399727e-02,  2.24278465e-01,\n",
              "         -7.26394877e-02, -6.90226778e-02,  2.45686576e-01,\n",
              "         -1.16721205e-02, -5.90202026e-02, -5.63130900e-02,\n",
              "          1.96493939e-01, -8.02434608e-02,  1.78033605e-01,\n",
              "          1.53375730e-01, -8.04285184e-02, -9.15076360e-02,\n",
              "          1.89396754e-01],\n",
              "        [ 1.84430733e-01, -7.89456815e-02,  1.87927246e-01,\n",
              "         -7.71476179e-02, -6.27894327e-02,  1.58390373e-01,\n",
              "         -1.06343189e-02, -1.15091383e-01, -7.10757822e-02,\n",
              "          1.88797355e-01, -8.19961056e-02,  1.47731543e-01,\n",
              "          2.37135887e-01, -8.34207907e-02, -4.57974672e-02,\n",
              "          2.29174361e-01],\n",
              "        [ 2.02092528e-01, -1.40640423e-01,  2.36223221e-01,\n",
              "         -9.01363790e-02, -9.21758488e-02,  2.04426423e-01,\n",
              "         -4.65806089e-02, -9.02674273e-02, -1.39789283e-01,\n",
              "          1.73410878e-01, -1.15890987e-01,  1.91196740e-01,\n",
              "          2.18252987e-01, -1.26869306e-01, -9.34804082e-02,\n",
              "          2.36947596e-01],\n",
              "        [ 1.70780763e-01,  2.32944321e-02,  1.92180276e-01,\n",
              "          3.12071089e-02, -3.44294542e-03,  2.17458218e-01,\n",
              "          1.37536963e-02,  2.35475637e-02,  4.74666655e-02,\n",
              "          1.54179856e-01,  3.55960876e-02,  2.09071025e-01,\n",
              "          1.45240188e-01,  5.55211306e-02,  6.52537076e-03,\n",
              "          2.06273049e-01],\n",
              "        [-4.49853949e-02,  2.18049318e-01, -6.01883531e-02,\n",
              "          1.50018200e-01,  2.42160589e-01, -7.05400780e-02,\n",
              "          2.32172221e-01,  1.78235322e-01,  1.77649513e-01,\n",
              "         -7.07256645e-02,  2.03610718e-01, -7.05345273e-02,\n",
              "         -5.73213883e-02,  2.22716123e-01,  1.71651229e-01,\n",
              "         -5.30725718e-02],\n",
              "        [ 1.69994384e-01, -1.19953401e-01,  2.29141742e-01,\n",
              "         -1.38490215e-01, -1.19569406e-01,  2.41804555e-01,\n",
              "         -6.85067400e-02, -1.41770929e-01, -1.50483161e-01,\n",
              "          1.78129464e-01, -1.55621871e-01,  1.67675495e-01,\n",
              "          1.69477746e-01, -1.50277629e-01, -1.36046126e-01,\n",
              "          2.16491193e-01],\n",
              "        [-2.21768506e-02,  1.45418376e-01,  2.33093835e-03,\n",
              "          1.31944388e-01,  1.47490367e-01, -1.39598427e-02,\n",
              "          2.35737219e-01,  1.43274307e-01,  1.90516725e-01,\n",
              "         -4.06274088e-02,  2.23680943e-01, -4.63458374e-02,\n",
              "         -8.87297187e-03,  1.91631362e-01,  1.91122353e-01,\n",
              "         -4.15333882e-02],\n",
              "        [ 4.45327796e-02,  1.89555109e-01,  5.23573607e-02,\n",
              "          1.07483938e-01,  1.81624219e-01,  3.46067362e-02,\n",
              "          1.42613351e-01,  2.03077435e-01,  2.11140931e-01,\n",
              "          5.87473810e-02,  1.44412339e-01,  5.42890541e-02,\n",
              "          3.54580395e-02,  2.17823654e-01,  2.15781197e-01,\n",
              "          3.94086912e-02],\n",
              "        [ 2.27026209e-01, -1.46840932e-02,  2.41905376e-01,\n",
              "         -1.41389649e-02,  1.45962555e-02,  1.88653529e-01,\n",
              "         -3.50047350e-02, -3.74230817e-02, -1.61453001e-02,\n",
              "          1.79570004e-01, -2.85029709e-02,  2.59472132e-01,\n",
              "          2.01459825e-01, -1.45205660e-02,  2.61569154e-02,\n",
              "          1.62452802e-01]], dtype=float32),\n",
              " array([0.194169  , 0.11345289, 0.17024377, 0.10197232, 0.08999691,\n",
              "        0.18977223, 0.04862954, 0.1096659 , 0.11804233, 0.17926855,\n",
              "        0.10837207, 0.1935801 , 0.17560531, 0.12077954, 0.11725209,\n",
              "        0.16691402], dtype=float32),\n",
              " array([[-0.21531524],\n",
              "        [ 0.18880382],\n",
              "        [-0.2450509 ],\n",
              "        [ 0.22607431],\n",
              "        [ 0.22787082],\n",
              "        [-0.21068554],\n",
              "        [ 0.27960604],\n",
              "        [ 0.21799542],\n",
              "        [ 0.20134975],\n",
              "        [-0.23243585],\n",
              "        [ 0.20366214],\n",
              "        [-0.22099678],\n",
              "        [-0.23915814],\n",
              "        [ 0.1862998 ],\n",
              "        [ 0.19629869],\n",
              "        [-0.24585308]], dtype=float32),\n",
              " array([-0.10281052], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4wrIAsFRCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wf = []\n",
        "wf.append(classifierA.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihs83CuJv6w",
        "colab_type": "text"
      },
      "source": [
        "**BOB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetYaBQYJdWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Btrain = sc.fit_transform(X_Btrain)\n",
        "#X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2vqdToWJdZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the ANN\n",
        "classifierB = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N05v4A_DJdev",
        "colab_type": "code",
        "outputId": "1cebbf61-a7fa-40f0-bfa9-36bfdc5e98f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifierB.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierB.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRuma_dbJdh_",
        "colab_type": "code",
        "outputId": "106a7afc-073d-4622-842a-f456ea229c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the second hidden layer\n",
        "classifierB.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierB.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9JhzEh-Jdk2",
        "colab_type": "code",
        "outputId": "7ef9e0f7-caf2-4f7a-cce6-b96654dda9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Adding the output layer\n",
        "classifierB.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcuaxjONJdnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the ANN\n",
        "classifierB.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9GY9jntJds0",
        "colab_type": "code",
        "outputId": "4cbc118b-43b4-4e5a-9917-6e42055f88b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting the ANN to the Training set\n",
        "classifierB.fit(X_Btrain, Y_Btrain, batch_size=100, nb_epoch=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5512\n",
            "Epoch 2/150\n",
            "127/127 [==============================] - 0s 52us/step - loss: 0.6925 - accuracy: 0.5984\n",
            "Epoch 3/150\n",
            "127/127 [==============================] - 0s 47us/step - loss: 0.6919 - accuracy: 0.5984\n",
            "Epoch 4/150\n",
            "127/127 [==============================] - 0s 117us/step - loss: 0.6912 - accuracy: 0.5984\n",
            "Epoch 5/150\n",
            "127/127 [==============================] - 0s 68us/step - loss: 0.6905 - accuracy: 0.5984\n",
            "Epoch 6/150\n",
            "127/127 [==============================] - 0s 87us/step - loss: 0.6898 - accuracy: 0.5984\n",
            "Epoch 7/150\n",
            "127/127 [==============================] - 0s 85us/step - loss: 0.6888 - accuracy: 0.5984\n",
            "Epoch 8/150\n",
            "127/127 [==============================] - 0s 79us/step - loss: 0.6876 - accuracy: 0.5984\n",
            "Epoch 9/150\n",
            "127/127 [==============================] - 0s 79us/step - loss: 0.6863 - accuracy: 0.5984\n",
            "Epoch 10/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.6847 - accuracy: 0.5984\n",
            "Epoch 11/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.6833 - accuracy: 0.5984\n",
            "Epoch 12/150\n",
            "127/127 [==============================] - 0s 90us/step - loss: 0.6807 - accuracy: 0.5984\n",
            "Epoch 13/150\n",
            "127/127 [==============================] - 0s 75us/step - loss: 0.6781 - accuracy: 0.5984\n",
            "Epoch 14/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.6761 - accuracy: 0.6142\n",
            "Epoch 15/150\n",
            "127/127 [==============================] - 0s 115us/step - loss: 0.6722 - accuracy: 0.6142\n",
            "Epoch 16/150\n",
            "127/127 [==============================] - 0s 68us/step - loss: 0.6690 - accuracy: 0.6299\n",
            "Epoch 17/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.6646 - accuracy: 0.6693\n",
            "Epoch 18/150\n",
            "127/127 [==============================] - 0s 74us/step - loss: 0.6603 - accuracy: 0.7087\n",
            "Epoch 19/150\n",
            "127/127 [==============================] - 0s 94us/step - loss: 0.6541 - accuracy: 0.7244\n",
            "Epoch 20/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.6487 - accuracy: 0.7559\n",
            "Epoch 21/150\n",
            "127/127 [==============================] - 0s 66us/step - loss: 0.6408 - accuracy: 0.8110\n",
            "Epoch 22/150\n",
            "127/127 [==============================] - 0s 80us/step - loss: 0.6327 - accuracy: 0.8346\n",
            "Epoch 23/150\n",
            "127/127 [==============================] - 0s 96us/step - loss: 0.6250 - accuracy: 0.8661\n",
            "Epoch 24/150\n",
            "127/127 [==============================] - 0s 69us/step - loss: 0.6132 - accuracy: 0.9134\n",
            "Epoch 25/150\n",
            "127/127 [==============================] - 0s 69us/step - loss: 0.6060 - accuracy: 0.8976\n",
            "Epoch 26/150\n",
            "127/127 [==============================] - 0s 121us/step - loss: 0.5914 - accuracy: 0.9055\n",
            "Epoch 27/150\n",
            "127/127 [==============================] - 0s 86us/step - loss: 0.5788 - accuracy: 0.9291\n",
            "Epoch 28/150\n",
            "127/127 [==============================] - 0s 81us/step - loss: 0.5661 - accuracy: 0.9370\n",
            "Epoch 29/150\n",
            "127/127 [==============================] - 0s 82us/step - loss: 0.5558 - accuracy: 0.9055\n",
            "Epoch 30/150\n",
            "127/127 [==============================] - 0s 96us/step - loss: 0.5343 - accuracy: 0.9370\n",
            "Epoch 31/150\n",
            "127/127 [==============================] - 0s 82us/step - loss: 0.5228 - accuracy: 0.9449\n",
            "Epoch 32/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.5068 - accuracy: 0.9449\n",
            "Epoch 33/150\n",
            "127/127 [==============================] - 0s 73us/step - loss: 0.4879 - accuracy: 0.9449\n",
            "Epoch 34/150\n",
            "127/127 [==============================] - 0s 84us/step - loss: 0.4671 - accuracy: 0.9370\n",
            "Epoch 35/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.4509 - accuracy: 0.9370\n",
            "Epoch 36/150\n",
            "127/127 [==============================] - 0s 58us/step - loss: 0.4373 - accuracy: 0.9449\n",
            "Epoch 37/150\n",
            "127/127 [==============================] - 0s 69us/step - loss: 0.4145 - accuracy: 0.9449\n",
            "Epoch 38/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.3984 - accuracy: 0.9449\n",
            "Epoch 39/150\n",
            "127/127 [==============================] - 0s 79us/step - loss: 0.3762 - accuracy: 0.9370\n",
            "Epoch 40/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.3584 - accuracy: 0.9449\n",
            "Epoch 41/150\n",
            "127/127 [==============================] - 0s 75us/step - loss: 0.3413 - accuracy: 0.9449\n",
            "Epoch 42/150\n",
            "127/127 [==============================] - 0s 73us/step - loss: 0.3321 - accuracy: 0.9449\n",
            "Epoch 43/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.3153 - accuracy: 0.9528\n",
            "Epoch 44/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.2994 - accuracy: 0.9449\n",
            "Epoch 45/150\n",
            "127/127 [==============================] - 0s 97us/step - loss: 0.2838 - accuracy: 0.9449\n",
            "Epoch 46/150\n",
            "127/127 [==============================] - 0s 81us/step - loss: 0.2732 - accuracy: 0.9449\n",
            "Epoch 47/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.2592 - accuracy: 0.9449\n",
            "Epoch 48/150\n",
            "127/127 [==============================] - 0s 70us/step - loss: 0.2442 - accuracy: 0.9449\n",
            "Epoch 49/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.2379 - accuracy: 0.9449\n",
            "Epoch 50/150\n",
            "127/127 [==============================] - 0s 87us/step - loss: 0.2287 - accuracy: 0.9449\n",
            "Epoch 51/150\n",
            "127/127 [==============================] - 0s 80us/step - loss: 0.2168 - accuracy: 0.9528\n",
            "Epoch 52/150\n",
            "127/127 [==============================] - 0s 81us/step - loss: 0.2176 - accuracy: 0.9449\n",
            "Epoch 53/150\n",
            "127/127 [==============================] - 0s 120us/step - loss: 0.1919 - accuracy: 0.9528\n",
            "Epoch 54/150\n",
            "127/127 [==============================] - 0s 77us/step - loss: 0.1849 - accuracy: 0.9528\n",
            "Epoch 55/150\n",
            "127/127 [==============================] - 0s 145us/step - loss: 0.1905 - accuracy: 0.9685\n",
            "Epoch 56/150\n",
            "127/127 [==============================] - 0s 95us/step - loss: 0.1748 - accuracy: 0.9606\n",
            "Epoch 57/150\n",
            "127/127 [==============================] - 0s 84us/step - loss: 0.1710 - accuracy: 0.9685\n",
            "Epoch 58/150\n",
            "127/127 [==============================] - 0s 109us/step - loss: 0.1712 - accuracy: 0.9606\n",
            "Epoch 59/150\n",
            "127/127 [==============================] - 0s 109us/step - loss: 0.1586 - accuracy: 0.9764\n",
            "Epoch 60/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.1586 - accuracy: 0.9764\n",
            "Epoch 61/150\n",
            "127/127 [==============================] - 0s 85us/step - loss: 0.1419 - accuracy: 0.9764\n",
            "Epoch 62/150\n",
            "127/127 [==============================] - 0s 139us/step - loss: 0.1446 - accuracy: 0.9843\n",
            "Epoch 63/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.1369 - accuracy: 0.9843\n",
            "Epoch 64/150\n",
            "127/127 [==============================] - 0s 88us/step - loss: 0.1401 - accuracy: 0.9764\n",
            "Epoch 65/150\n",
            "127/127 [==============================] - 0s 69us/step - loss: 0.1293 - accuracy: 0.9843\n",
            "Epoch 66/150\n",
            "127/127 [==============================] - 0s 77us/step - loss: 0.1374 - accuracy: 0.9843\n",
            "Epoch 67/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.1247 - accuracy: 0.9843\n",
            "Epoch 68/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.1202 - accuracy: 0.9843\n",
            "Epoch 69/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.1238 - accuracy: 0.9843\n",
            "Epoch 70/150\n",
            "127/127 [==============================] - 0s 73us/step - loss: 0.1183 - accuracy: 0.9764\n",
            "Epoch 71/150\n",
            "127/127 [==============================] - 0s 66us/step - loss: 0.1099 - accuracy: 0.9843\n",
            "Epoch 72/150\n",
            "127/127 [==============================] - 0s 90us/step - loss: 0.1109 - accuracy: 0.9843\n",
            "Epoch 73/150\n",
            "127/127 [==============================] - 0s 73us/step - loss: 0.1125 - accuracy: 0.9843\n",
            "Epoch 74/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.1066 - accuracy: 0.9843\n",
            "Epoch 75/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.1044 - accuracy: 0.9843\n",
            "Epoch 76/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.1057 - accuracy: 0.9843\n",
            "Epoch 77/150\n",
            "127/127 [==============================] - 0s 62us/step - loss: 0.1001 - accuracy: 0.9843\n",
            "Epoch 78/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.0962 - accuracy: 0.9843\n",
            "Epoch 79/150\n",
            "127/127 [==============================] - 0s 68us/step - loss: 0.0976 - accuracy: 0.9843\n",
            "Epoch 80/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.0938 - accuracy: 0.9843\n",
            "Epoch 81/150\n",
            "127/127 [==============================] - 0s 65us/step - loss: 0.0915 - accuracy: 0.9843\n",
            "Epoch 82/150\n",
            "127/127 [==============================] - 0s 80us/step - loss: 0.0861 - accuracy: 0.9843\n",
            "Epoch 83/150\n",
            "127/127 [==============================] - 0s 94us/step - loss: 0.0941 - accuracy: 0.9843\n",
            "Epoch 84/150\n",
            "127/127 [==============================] - 0s 73us/step - loss: 0.0910 - accuracy: 0.9843\n",
            "Epoch 85/150\n",
            "127/127 [==============================] - 0s 88us/step - loss: 0.0892 - accuracy: 0.9921\n",
            "Epoch 86/150\n",
            "127/127 [==============================] - 0s 61us/step - loss: 0.0938 - accuracy: 0.9921\n",
            "Epoch 87/150\n",
            "127/127 [==============================] - 0s 135us/step - loss: 0.0837 - accuracy: 0.9921\n",
            "Epoch 88/150\n",
            "127/127 [==============================] - 0s 70us/step - loss: 0.0863 - accuracy: 0.9921\n",
            "Epoch 89/150\n",
            "127/127 [==============================] - 0s 89us/step - loss: 0.0843 - accuracy: 0.9921\n",
            "Epoch 90/150\n",
            "127/127 [==============================] - 0s 102us/step - loss: 0.0834 - accuracy: 0.9843\n",
            "Epoch 91/150\n",
            "127/127 [==============================] - 0s 146us/step - loss: 0.0824 - accuracy: 0.9921\n",
            "Epoch 92/150\n",
            "127/127 [==============================] - 0s 105us/step - loss: 0.0674 - accuracy: 0.9843\n",
            "Epoch 93/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.0710 - accuracy: 0.9921\n",
            "Epoch 94/150\n",
            "127/127 [==============================] - 0s 88us/step - loss: 0.0801 - accuracy: 0.9843\n",
            "Epoch 95/150\n",
            "127/127 [==============================] - 0s 85us/step - loss: 0.0793 - accuracy: 0.9843\n",
            "Epoch 96/150\n",
            "127/127 [==============================] - 0s 76us/step - loss: 0.0723 - accuracy: 0.9921\n",
            "Epoch 97/150\n",
            "127/127 [==============================] - 0s 74us/step - loss: 0.0693 - accuracy: 0.9921\n",
            "Epoch 98/150\n",
            "127/127 [==============================] - 0s 78us/step - loss: 0.0668 - accuracy: 0.9921\n",
            "Epoch 99/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.0695 - accuracy: 0.9921\n",
            "Epoch 100/150\n",
            "127/127 [==============================] - 0s 83us/step - loss: 0.0647 - accuracy: 0.9921\n",
            "Epoch 101/150\n",
            "127/127 [==============================] - 0s 72us/step - loss: 0.0657 - accuracy: 0.9921\n",
            "Epoch 102/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.0670 - accuracy: 0.9921\n",
            "Epoch 103/150\n",
            "127/127 [==============================] - 0s 71us/step - loss: 0.0692 - accuracy: 0.9921\n",
            "Epoch 104/150\n",
            "127/127 [==============================] - 0s 194us/step - loss: 0.0655 - accuracy: 0.9921\n",
            "Epoch 105/150\n",
            "127/127 [==============================] - 0s 118us/step - loss: 0.0681 - accuracy: 0.9921\n",
            "Epoch 106/150\n",
            "127/127 [==============================] - 0s 144us/step - loss: 0.0676 - accuracy: 0.9921\n",
            "Epoch 107/150\n",
            "127/127 [==============================] - 0s 219us/step - loss: 0.0599 - accuracy: 0.9921\n",
            "Epoch 108/150\n",
            "127/127 [==============================] - 0s 168us/step - loss: 0.0608 - accuracy: 0.9921\n",
            "Epoch 109/150\n",
            "127/127 [==============================] - 0s 118us/step - loss: 0.0590 - accuracy: 0.9921\n",
            "Epoch 110/150\n",
            "127/127 [==============================] - 0s 63us/step - loss: 0.0582 - accuracy: 0.9921\n",
            "Epoch 111/150\n",
            "127/127 [==============================] - 0s 52us/step - loss: 0.0576 - accuracy: 0.9921\n",
            "Epoch 112/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0529 - accuracy: 0.9921\n",
            "Epoch 113/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0513 - accuracy: 0.9921\n",
            "Epoch 114/150\n",
            "127/127 [==============================] - 0s 51us/step - loss: 0.0535 - accuracy: 0.9921\n",
            "Epoch 115/150\n",
            "127/127 [==============================] - 0s 52us/step - loss: 0.0591 - accuracy: 0.9921\n",
            "Epoch 116/150\n",
            "127/127 [==============================] - 0s 63us/step - loss: 0.0610 - accuracy: 0.9921\n",
            "Epoch 117/150\n",
            "127/127 [==============================] - 0s 61us/step - loss: 0.0544 - accuracy: 0.9921\n",
            "Epoch 118/150\n",
            "127/127 [==============================] - 0s 50us/step - loss: 0.0536 - accuracy: 0.9921\n",
            "Epoch 119/150\n",
            "127/127 [==============================] - 0s 87us/step - loss: 0.0535 - accuracy: 0.9921\n",
            "Epoch 120/150\n",
            "127/127 [==============================] - 0s 83us/step - loss: 0.0546 - accuracy: 0.9921\n",
            "Epoch 121/150\n",
            "127/127 [==============================] - 0s 65us/step - loss: 0.0498 - accuracy: 0.9921\n",
            "Epoch 122/150\n",
            "127/127 [==============================] - 0s 46us/step - loss: 0.0538 - accuracy: 0.9921\n",
            "Epoch 123/150\n",
            "127/127 [==============================] - 0s 51us/step - loss: 0.0484 - accuracy: 0.9921\n",
            "Epoch 124/150\n",
            "127/127 [==============================] - 0s 52us/step - loss: 0.0463 - accuracy: 0.9921\n",
            "Epoch 125/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0478 - accuracy: 0.9921\n",
            "Epoch 126/150\n",
            "127/127 [==============================] - 0s 44us/step - loss: 0.0455 - accuracy: 0.9921\n",
            "Epoch 127/150\n",
            "127/127 [==============================] - 0s 49us/step - loss: 0.0471 - accuracy: 0.9921\n",
            "Epoch 128/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0518 - accuracy: 0.9921\n",
            "Epoch 129/150\n",
            "127/127 [==============================] - 0s 47us/step - loss: 0.0474 - accuracy: 0.9921\n",
            "Epoch 130/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0434 - accuracy: 0.9921\n",
            "Epoch 131/150\n",
            "127/127 [==============================] - 0s 45us/step - loss: 0.0430 - accuracy: 0.9921\n",
            "Epoch 132/150\n",
            "127/127 [==============================] - 0s 46us/step - loss: 0.0436 - accuracy: 0.9921\n",
            "Epoch 133/150\n",
            "127/127 [==============================] - 0s 44us/step - loss: 0.0482 - accuracy: 0.9921\n",
            "Epoch 134/150\n",
            "127/127 [==============================] - 0s 48us/step - loss: 0.0478 - accuracy: 0.9921\n",
            "Epoch 135/150\n",
            "127/127 [==============================] - 0s 50us/step - loss: 0.0431 - accuracy: 0.9921\n",
            "Epoch 136/150\n",
            "127/127 [==============================] - 0s 53us/step - loss: 0.0398 - accuracy: 0.9921\n",
            "Epoch 137/150\n",
            "127/127 [==============================] - 0s 49us/step - loss: 0.0407 - accuracy: 0.9921\n",
            "Epoch 138/150\n",
            "127/127 [==============================] - 0s 46us/step - loss: 0.0456 - accuracy: 0.9921\n",
            "Epoch 139/150\n",
            "127/127 [==============================] - 0s 54us/step - loss: 0.0415 - accuracy: 0.9921\n",
            "Epoch 140/150\n",
            "127/127 [==============================] - 0s 49us/step - loss: 0.0393 - accuracy: 0.9921\n",
            "Epoch 141/150\n",
            "127/127 [==============================] - 0s 64us/step - loss: 0.0357 - accuracy: 0.9921\n",
            "Epoch 142/150\n",
            "127/127 [==============================] - 0s 58us/step - loss: 0.0432 - accuracy: 0.9921\n",
            "Epoch 143/150\n",
            "127/127 [==============================] - 0s 51us/step - loss: 0.0352 - accuracy: 0.9921\n",
            "Epoch 144/150\n",
            "127/127 [==============================] - 0s 56us/step - loss: 0.0416 - accuracy: 0.9921\n",
            "Epoch 145/150\n",
            "127/127 [==============================] - 0s 69us/step - loss: 0.0376 - accuracy: 0.9921\n",
            "Epoch 146/150\n",
            "127/127 [==============================] - 0s 62us/step - loss: 0.0354 - accuracy: 0.9921\n",
            "Epoch 147/150\n",
            "127/127 [==============================] - 0s 60us/step - loss: 0.0343 - accuracy: 0.9921\n",
            "Epoch 148/150\n",
            "127/127 [==============================] - 0s 58us/step - loss: 0.0394 - accuracy: 0.9921\n",
            "Epoch 149/150\n",
            "127/127 [==============================] - 0s 61us/step - loss: 0.0358 - accuracy: 0.9921\n",
            "Epoch 150/150\n",
            "127/127 [==============================] - 0s 54us/step - loss: 0.0334 - accuracy: 0.9921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f399e3ef588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3KPWaqZJdqs",
        "colab_type": "code",
        "outputId": "b45b223c-a984-4c4f-d070-fb5bfd60bb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_Bpred = classifierB.predict(X_test)\n",
        "y_Bpred = (y_Bpred > 0.5)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(y_Bpred))*100\n",
        "cm = confusion_matrix(y_test, np.round(y_Bpred))\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX')\n",
        "print(cm)\n",
        "\n",
        "print('\\nTEST METRICS')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX\n",
            "[[48  5]\n",
            " [ 2 21]]\n",
            "\n",
            "TEST METRICS\n",
            "Accuracy: 90.78947368421053%\n",
            "Precision: 80.76923076923077%\n",
            "Recall: 91.30434782608695%\n",
            "F1-score: 85.71428571428572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qs4PTquJdcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wf.append(classifierB.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OEuZPe2KKVD",
        "colab_type": "text"
      },
      "source": [
        "**Chips**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FCvYNDDKDbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Ctrain = sc.fit_transform(X_Ctrain)\n",
        "#X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhx9-HTTKDSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the ANN\n",
        "classifierC = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il3BTnJSKDPi",
        "colab_type": "code",
        "outputId": "27755f70-5a24-47c5-92a3-38d9211b8db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifierC.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierC.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7PTiIY3KR_h",
        "colab_type": "code",
        "outputId": "0034aefd-03c1-4241-c6db-5e25d3198b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the second hidden layer\n",
        "classifierC.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierC.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRlHvbNKSCe",
        "colab_type": "code",
        "outputId": "c3adab1f-7bba-46ff-a19d-c88313ab532d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Adding the output layer\n",
        "classifierC.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4hexqE-KSFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the ANN\n",
        "classifierC.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfTAyS_mKSIS",
        "colab_type": "code",
        "outputId": "26a9d3cd-d911-4ef6-880d-cefd259fb5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting the ANN to the Training set\n",
        "classifierC.fit(X_Ctrain, Y_Ctrain, batch_size=100, nb_epoch=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.3200\n",
            "Epoch 2/150\n",
            "50/50 [==============================] - 0s 40us/step - loss: 0.6931 - accuracy: 0.5800\n",
            "Epoch 3/150\n",
            "50/50 [==============================] - 0s 58us/step - loss: 0.6928 - accuracy: 0.6400\n",
            "Epoch 4/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.6924 - accuracy: 0.6600\n",
            "Epoch 5/150\n",
            "50/50 [==============================] - 0s 42us/step - loss: 0.6921 - accuracy: 0.6600\n",
            "Epoch 6/150\n",
            "50/50 [==============================] - 0s 33us/step - loss: 0.6917 - accuracy: 0.6400\n",
            "Epoch 7/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.6913 - accuracy: 0.6600\n",
            "Epoch 8/150\n",
            "50/50 [==============================] - 0s 34us/step - loss: 0.6910 - accuracy: 0.6200\n",
            "Epoch 9/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6904 - accuracy: 0.6600\n",
            "Epoch 10/150\n",
            "50/50 [==============================] - 0s 34us/step - loss: 0.6899 - accuracy: 0.7000\n",
            "Epoch 11/150\n",
            "50/50 [==============================] - 0s 46us/step - loss: 0.6893 - accuracy: 0.7000\n",
            "Epoch 12/150\n",
            "50/50 [==============================] - 0s 34us/step - loss: 0.6887 - accuracy: 0.7200\n",
            "Epoch 13/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6882 - accuracy: 0.7600\n",
            "Epoch 14/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.6874 - accuracy: 0.7200\n",
            "Epoch 15/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.6868 - accuracy: 0.7600\n",
            "Epoch 16/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.6861 - accuracy: 0.7600\n",
            "Epoch 17/150\n",
            "50/50 [==============================] - 0s 54us/step - loss: 0.6847 - accuracy: 0.8200\n",
            "Epoch 18/150\n",
            "50/50 [==============================] - 0s 58us/step - loss: 0.6834 - accuracy: 0.8000\n",
            "Epoch 19/150\n",
            "50/50 [==============================] - 0s 57us/step - loss: 0.6826 - accuracy: 0.8000\n",
            "Epoch 20/150\n",
            "50/50 [==============================] - 0s 57us/step - loss: 0.6814 - accuracy: 0.8400\n",
            "Epoch 21/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.6803 - accuracy: 0.8400\n",
            "Epoch 22/150\n",
            "50/50 [==============================] - 0s 116us/step - loss: 0.6788 - accuracy: 0.8600\n",
            "Epoch 23/150\n",
            "50/50 [==============================] - 0s 39us/step - loss: 0.6778 - accuracy: 0.8400\n",
            "Epoch 24/150\n",
            "50/50 [==============================] - 0s 33us/step - loss: 0.6763 - accuracy: 0.8400\n",
            "Epoch 25/150\n",
            "50/50 [==============================] - 0s 31us/step - loss: 0.6739 - accuracy: 0.8400\n",
            "Epoch 26/150\n",
            "50/50 [==============================] - 0s 37us/step - loss: 0.6728 - accuracy: 0.9200\n",
            "Epoch 27/150\n",
            "50/50 [==============================] - 0s 64us/step - loss: 0.6695 - accuracy: 0.8800\n",
            "Epoch 28/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.6681 - accuracy: 0.9000\n",
            "Epoch 29/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.6669 - accuracy: 0.9200\n",
            "Epoch 30/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6628 - accuracy: 0.9200\n",
            "Epoch 31/150\n",
            "50/50 [==============================] - 0s 29us/step - loss: 0.6613 - accuracy: 0.9200\n",
            "Epoch 32/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6566 - accuracy: 0.9200\n",
            "Epoch 33/150\n",
            "50/50 [==============================] - 0s 31us/step - loss: 0.6552 - accuracy: 0.9400\n",
            "Epoch 34/150\n",
            "50/50 [==============================] - 0s 30us/step - loss: 0.6522 - accuracy: 0.9000\n",
            "Epoch 35/150\n",
            "50/50 [==============================] - 0s 29us/step - loss: 0.6485 - accuracy: 0.9600\n",
            "Epoch 36/150\n",
            "50/50 [==============================] - 0s 28us/step - loss: 0.6454 - accuracy: 0.9400\n",
            "Epoch 37/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.6423 - accuracy: 0.9600\n",
            "Epoch 38/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.6378 - accuracy: 0.9600\n",
            "Epoch 39/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6326 - accuracy: 0.9600\n",
            "Epoch 40/150\n",
            "50/50 [==============================] - 0s 30us/step - loss: 0.6343 - accuracy: 0.9600\n",
            "Epoch 41/150\n",
            "50/50 [==============================] - 0s 81us/step - loss: 0.6246 - accuracy: 0.9600\n",
            "Epoch 42/150\n",
            "50/50 [==============================] - 0s 36us/step - loss: 0.6205 - accuracy: 0.9600\n",
            "Epoch 43/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.6162 - accuracy: 0.9400\n",
            "Epoch 44/150\n",
            "50/50 [==============================] - 0s 28us/step - loss: 0.6096 - accuracy: 0.9600\n",
            "Epoch 45/150\n",
            "50/50 [==============================] - 0s 31us/step - loss: 0.6037 - accuracy: 0.9600\n",
            "Epoch 46/150\n",
            "50/50 [==============================] - 0s 46us/step - loss: 0.5982 - accuracy: 0.9600\n",
            "Epoch 47/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.5910 - accuracy: 0.9600\n",
            "Epoch 48/150\n",
            "50/50 [==============================] - 0s 31us/step - loss: 0.5906 - accuracy: 0.9600\n",
            "Epoch 49/150\n",
            "50/50 [==============================] - 0s 34us/step - loss: 0.5808 - accuracy: 0.9600\n",
            "Epoch 50/150\n",
            "50/50 [==============================] - 0s 55us/step - loss: 0.5740 - accuracy: 0.9600\n",
            "Epoch 51/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.5651 - accuracy: 0.9600\n",
            "Epoch 52/150\n",
            "50/50 [==============================] - 0s 28us/step - loss: 0.5606 - accuracy: 0.9600\n",
            "Epoch 53/150\n",
            "50/50 [==============================] - 0s 31us/step - loss: 0.5545 - accuracy: 0.9600\n",
            "Epoch 54/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.5417 - accuracy: 0.9600\n",
            "Epoch 55/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.5374 - accuracy: 0.9600\n",
            "Epoch 56/150\n",
            "50/50 [==============================] - 0s 32us/step - loss: 0.5322 - accuracy: 0.9400\n",
            "Epoch 57/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.5207 - accuracy: 0.9600\n",
            "Epoch 58/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.5190 - accuracy: 0.9400\n",
            "Epoch 59/150\n",
            "50/50 [==============================] - 0s 47us/step - loss: 0.4973 - accuracy: 0.9400\n",
            "Epoch 60/150\n",
            "50/50 [==============================] - 0s 103us/step - loss: 0.5058 - accuracy: 0.9400\n",
            "Epoch 61/150\n",
            "50/50 [==============================] - 0s 46us/step - loss: 0.4943 - accuracy: 0.9400\n",
            "Epoch 62/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.4845 - accuracy: 0.9600\n",
            "Epoch 63/150\n",
            "50/50 [==============================] - 0s 40us/step - loss: 0.4783 - accuracy: 0.9600\n",
            "Epoch 64/150\n",
            "50/50 [==============================] - 0s 43us/step - loss: 0.4626 - accuracy: 0.9600\n",
            "Epoch 65/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.4466 - accuracy: 0.9600\n",
            "Epoch 66/150\n",
            "50/50 [==============================] - 0s 39us/step - loss: 0.4421 - accuracy: 0.9800\n",
            "Epoch 67/150\n",
            "50/50 [==============================] - 0s 53us/step - loss: 0.4301 - accuracy: 0.9600\n",
            "Epoch 68/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.4315 - accuracy: 0.9600\n",
            "Epoch 69/150\n",
            "50/50 [==============================] - 0s 39us/step - loss: 0.4201 - accuracy: 0.9600\n",
            "Epoch 70/150\n",
            "50/50 [==============================] - 0s 42us/step - loss: 0.4166 - accuracy: 0.9800\n",
            "Epoch 71/150\n",
            "50/50 [==============================] - 0s 54us/step - loss: 0.4098 - accuracy: 0.9600\n",
            "Epoch 72/150\n",
            "50/50 [==============================] - 0s 40us/step - loss: 0.3985 - accuracy: 0.9800\n",
            "Epoch 73/150\n",
            "50/50 [==============================] - 0s 50us/step - loss: 0.3960 - accuracy: 0.9800\n",
            "Epoch 74/150\n",
            "50/50 [==============================] - 0s 43us/step - loss: 0.3736 - accuracy: 0.9800\n",
            "Epoch 75/150\n",
            "50/50 [==============================] - 0s 75us/step - loss: 0.3620 - accuracy: 0.9600\n",
            "Epoch 76/150\n",
            "50/50 [==============================] - 0s 65us/step - loss: 0.3519 - accuracy: 0.9800\n",
            "Epoch 77/150\n",
            "50/50 [==============================] - 0s 41us/step - loss: 0.3427 - accuracy: 0.9800\n",
            "Epoch 78/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.3437 - accuracy: 0.9600\n",
            "Epoch 79/150\n",
            "50/50 [==============================] - 0s 52us/step - loss: 0.3414 - accuracy: 0.9800\n",
            "Epoch 80/150\n",
            "50/50 [==============================] - 0s 43us/step - loss: 0.3199 - accuracy: 0.9600\n",
            "Epoch 81/150\n",
            "50/50 [==============================] - 0s 59us/step - loss: 0.3184 - accuracy: 0.9800\n",
            "Epoch 82/150\n",
            "50/50 [==============================] - 0s 49us/step - loss: 0.3177 - accuracy: 0.9600\n",
            "Epoch 83/150\n",
            "50/50 [==============================] - 0s 68us/step - loss: 0.3032 - accuracy: 0.9800\n",
            "Epoch 84/150\n",
            "50/50 [==============================] - 0s 71us/step - loss: 0.2908 - accuracy: 0.9800\n",
            "Epoch 85/150\n",
            "50/50 [==============================] - 0s 50us/step - loss: 0.2780 - accuracy: 0.9800\n",
            "Epoch 86/150\n",
            "50/50 [==============================] - 0s 45us/step - loss: 0.2738 - accuracy: 0.9800\n",
            "Epoch 87/150\n",
            "50/50 [==============================] - 0s 59us/step - loss: 0.2767 - accuracy: 0.9800\n",
            "Epoch 88/150\n",
            "50/50 [==============================] - 0s 43us/step - loss: 0.2665 - accuracy: 0.9800\n",
            "Epoch 89/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.2601 - accuracy: 0.9800\n",
            "Epoch 90/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.2494 - accuracy: 0.9800\n",
            "Epoch 91/150\n",
            "50/50 [==============================] - 0s 48us/step - loss: 0.2389 - accuracy: 0.9800\n",
            "Epoch 92/150\n",
            "50/50 [==============================] - 0s 43us/step - loss: 0.2428 - accuracy: 0.9800\n",
            "Epoch 93/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.2276 - accuracy: 0.9800\n",
            "Epoch 94/150\n",
            "50/50 [==============================] - 0s 57us/step - loss: 0.2251 - accuracy: 0.9800\n",
            "Epoch 95/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.2169 - accuracy: 0.9800\n",
            "Epoch 96/150\n",
            "50/50 [==============================] - 0s 39us/step - loss: 0.2160 - accuracy: 0.9800\n",
            "Epoch 97/150\n",
            "50/50 [==============================] - 0s 47us/step - loss: 0.2032 - accuracy: 0.9800\n",
            "Epoch 98/150\n",
            "50/50 [==============================] - 0s 56us/step - loss: 0.1950 - accuracy: 0.9800\n",
            "Epoch 99/150\n",
            "50/50 [==============================] - 0s 70us/step - loss: 0.1938 - accuracy: 0.9800\n",
            "Epoch 100/150\n",
            "50/50 [==============================] - 0s 46us/step - loss: 0.1826 - accuracy: 0.9800\n",
            "Epoch 101/150\n",
            "50/50 [==============================] - 0s 46us/step - loss: 0.1843 - accuracy: 0.9800\n",
            "Epoch 102/150\n",
            "50/50 [==============================] - 0s 64us/step - loss: 0.1801 - accuracy: 0.9800\n",
            "Epoch 103/150\n",
            "50/50 [==============================] - 0s 50us/step - loss: 0.1769 - accuracy: 0.9800\n",
            "Epoch 104/150\n",
            "50/50 [==============================] - 0s 71us/step - loss: 0.1681 - accuracy: 0.9800\n",
            "Epoch 105/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.1757 - accuracy: 0.9800\n",
            "Epoch 106/150\n",
            "50/50 [==============================] - 0s 54us/step - loss: 0.1595 - accuracy: 0.9800\n",
            "Epoch 107/150\n",
            "50/50 [==============================] - 0s 53us/step - loss: 0.1552 - accuracy: 0.9800\n",
            "Epoch 108/150\n",
            "50/50 [==============================] - 0s 67us/step - loss: 0.1544 - accuracy: 0.9800\n",
            "Epoch 109/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.1463 - accuracy: 0.9800\n",
            "Epoch 110/150\n",
            "50/50 [==============================] - 0s 68us/step - loss: 0.1477 - accuracy: 0.9800\n",
            "Epoch 111/150\n",
            "50/50 [==============================] - 0s 70us/step - loss: 0.1361 - accuracy: 1.0000\n",
            "Epoch 112/150\n",
            "50/50 [==============================] - 0s 66us/step - loss: 0.1399 - accuracy: 0.9800\n",
            "Epoch 113/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.1396 - accuracy: 0.9800\n",
            "Epoch 114/150\n",
            "50/50 [==============================] - 0s 52us/step - loss: 0.1420 - accuracy: 0.9800\n",
            "Epoch 115/150\n",
            "50/50 [==============================] - 0s 91us/step - loss: 0.1333 - accuracy: 0.9800\n",
            "Epoch 116/150\n",
            "50/50 [==============================] - 0s 35us/step - loss: 0.1198 - accuracy: 0.9800\n",
            "Epoch 117/150\n",
            "50/50 [==============================] - 0s 70us/step - loss: 0.1129 - accuracy: 0.9800\n",
            "Epoch 118/150\n",
            "50/50 [==============================] - 0s 67us/step - loss: 0.1256 - accuracy: 1.0000\n",
            "Epoch 119/150\n",
            "50/50 [==============================] - 0s 71us/step - loss: 0.1124 - accuracy: 1.0000\n",
            "Epoch 120/150\n",
            "50/50 [==============================] - 0s 61us/step - loss: 0.1099 - accuracy: 1.0000\n",
            "Epoch 121/150\n",
            "50/50 [==============================] - 0s 50us/step - loss: 0.1034 - accuracy: 0.9800\n",
            "Epoch 122/150\n",
            "50/50 [==============================] - 0s 38us/step - loss: 0.1069 - accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "50/50 [==============================] - 0s 42us/step - loss: 0.1073 - accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.1032 - accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "50/50 [==============================] - 0s 63us/step - loss: 0.1027 - accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "50/50 [==============================] - 0s 57us/step - loss: 0.0991 - accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "50/50 [==============================] - 0s 56us/step - loss: 0.0996 - accuracy: 0.9800\n",
            "Epoch 128/150\n",
            "50/50 [==============================] - 0s 58us/step - loss: 0.0906 - accuracy: 1.0000\n",
            "Epoch 129/150\n",
            "50/50 [==============================] - 0s 44us/step - loss: 0.0864 - accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "50/50 [==============================] - 0s 49us/step - loss: 0.0890 - accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "50/50 [==============================] - 0s 63us/step - loss: 0.0896 - accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "50/50 [==============================] - 0s 60us/step - loss: 0.0887 - accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "50/50 [==============================] - 0s 66us/step - loss: 0.0868 - accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "50/50 [==============================] - 0s 58us/step - loss: 0.0901 - accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "50/50 [==============================] - 0s 73us/step - loss: 0.0884 - accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "50/50 [==============================] - 0s 63us/step - loss: 0.0791 - accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "50/50 [==============================] - 0s 63us/step - loss: 0.0832 - accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "50/50 [==============================] - 0s 61us/step - loss: 0.0786 - accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "50/50 [==============================] - 0s 62us/step - loss: 0.0769 - accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "50/50 [==============================] - 0s 56us/step - loss: 0.0788 - accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "50/50 [==============================] - 0s 85us/step - loss: 0.0700 - accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "50/50 [==============================] - 0s 77us/step - loss: 0.0656 - accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "50/50 [==============================] - 0s 66us/step - loss: 0.0673 - accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "50/50 [==============================] - 0s 62us/step - loss: 0.0646 - accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "50/50 [==============================] - 0s 63us/step - loss: 0.0700 - accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "50/50 [==============================] - 0s 57us/step - loss: 0.0628 - accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "50/50 [==============================] - 0s 67us/step - loss: 0.0686 - accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "50/50 [==============================] - 0s 45us/step - loss: 0.0556 - accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "50/50 [==============================] - 0s 49us/step - loss: 0.0639 - accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "50/50 [==============================] - 0s 64us/step - loss: 0.0619 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f399e040a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck5p7_dCKSOe",
        "colab_type": "code",
        "outputId": "51c55c2c-c26d-4f7c-e36e-e30acd9354ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_Cpred = classifierC.predict(X_test)\n",
        "y_Cpred = (y_Cpred > 0.5)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(y_Cpred))*100\n",
        "cm = confusion_matrix(y_test, np.round(y_Cpred))\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX')\n",
        "print(cm)\n",
        "\n",
        "print('\\nTEST METRICS')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX\n",
            "[[48  5]\n",
            " [ 2 21]]\n",
            "\n",
            "TEST METRICS\n",
            "Accuracy: 90.78947368421053%\n",
            "Precision: 80.76923076923077%\n",
            "Recall: 91.30434782608695%\n",
            "F1-score: 85.71428571428572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZI_PaSEKSMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wf.append(classifierC.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GkbVPDvKmcm",
        "colab_type": "text"
      },
      "source": [
        "Federated model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKcsBgNXKiqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#averaging the weights\n",
        "import numpy\n",
        "New_weights = list()\n",
        "\n",
        "for weights_list_tuple in zip(*wf):\n",
        "    New_weights.append(\n",
        "        [numpy.array(wf_).mean(axis=0)\\\n",
        "            for wf_ in zip(*weights_list_tuple)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cg_6FxxKinJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.iloc[:, 2:].values\n",
        "y = data.iloc[:, 1].values\n",
        "\n",
        "# Encoding categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "y = labelencoder_X_1.fit_transform(y)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "#X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdvTAkM0KilU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSP9bdWVKihp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the ANN\n",
        "classifierF = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN4CRADeKifP",
        "colab_type": "code",
        "outputId": "52339591-6bbb-4e5d-df4d-2c07e5b876ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifierF.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierF.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duZlNBWYKick",
        "colab_type": "code",
        "outputId": "36c4b83e-400b-4577-ce4c-aaced0096692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Adding the second hidden layer\n",
        "classifierF.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifierF.add(Dropout(p=0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__otxg3eKiZn",
        "colab_type": "code",
        "outputId": "e0bf7e19-4380-4454-b210-f6625fbbca5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Adding the output layer\n",
        "classifierF.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29TRHeJHKiXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the ANN\n",
        "classifierF.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1HgaZCBKiUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifierF.set_weights(New_weights) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZ6dpNaKiQx",
        "colab_type": "code",
        "outputId": "4d9a2bf2-bb34-489e-8233-d5885eb6211b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting the ANN to the Training set\n",
        "classifierF.fit(X_train, y_train, batch_size=100, nb_epoch=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "303/303 [==============================] - 0s 467us/step - loss: 0.3951 - accuracy: 0.9703\n",
            "Epoch 2/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.3752 - accuracy: 0.9703\n",
            "Epoch 3/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.3494 - accuracy: 0.9769\n",
            "Epoch 4/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.3278 - accuracy: 0.9670\n",
            "Epoch 5/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.3045 - accuracy: 0.9703\n",
            "Epoch 6/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.2950 - accuracy: 0.9670\n",
            "Epoch 7/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.2748 - accuracy: 0.9703\n",
            "Epoch 8/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.2570 - accuracy: 0.9703\n",
            "Epoch 9/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.2384 - accuracy: 0.9637\n",
            "Epoch 10/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.2239 - accuracy: 0.9637\n",
            "Epoch 11/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.2094 - accuracy: 0.9736\n",
            "Epoch 12/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.1991 - accuracy: 0.9571\n",
            "Epoch 13/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.1888 - accuracy: 0.9670\n",
            "Epoch 14/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.1806 - accuracy: 0.9604\n",
            "Epoch 15/150\n",
            "303/303 [==============================] - 0s 36us/step - loss: 0.1762 - accuracy: 0.9637\n",
            "Epoch 16/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.1641 - accuracy: 0.9670\n",
            "Epoch 17/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.1532 - accuracy: 0.9703\n",
            "Epoch 18/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.1540 - accuracy: 0.9736\n",
            "Epoch 19/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.1491 - accuracy: 0.9802\n",
            "Epoch 20/150\n",
            "303/303 [==============================] - 0s 26us/step - loss: 0.1492 - accuracy: 0.9670\n",
            "Epoch 21/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.1469 - accuracy: 0.9703\n",
            "Epoch 22/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.1376 - accuracy: 0.9703\n",
            "Epoch 23/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.1379 - accuracy: 0.9670\n",
            "Epoch 24/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.1301 - accuracy: 0.9703\n",
            "Epoch 25/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.1273 - accuracy: 0.9769\n",
            "Epoch 26/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.1191 - accuracy: 0.9703\n",
            "Epoch 27/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.1266 - accuracy: 0.9670\n",
            "Epoch 28/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.1205 - accuracy: 0.9703\n",
            "Epoch 29/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.1144 - accuracy: 0.9736\n",
            "Epoch 30/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.1097 - accuracy: 0.9670\n",
            "Epoch 31/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.1092 - accuracy: 0.9736\n",
            "Epoch 32/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.1039 - accuracy: 0.9769\n",
            "Epoch 33/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0994 - accuracy: 0.9802\n",
            "Epoch 34/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.1061 - accuracy: 0.9670\n",
            "Epoch 35/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.1048 - accuracy: 0.9703\n",
            "Epoch 36/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.1005 - accuracy: 0.9703\n",
            "Epoch 37/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.1023 - accuracy: 0.9670\n",
            "Epoch 38/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.1035 - accuracy: 0.9703\n",
            "Epoch 39/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0969 - accuracy: 0.9703\n",
            "Epoch 40/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0978 - accuracy: 0.9670\n",
            "Epoch 41/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0989 - accuracy: 0.9703\n",
            "Epoch 42/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0933 - accuracy: 0.9769\n",
            "Epoch 43/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0974 - accuracy: 0.9769\n",
            "Epoch 44/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0933 - accuracy: 0.9802\n",
            "Epoch 45/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0954 - accuracy: 0.9703\n",
            "Epoch 46/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.0942 - accuracy: 0.9703\n",
            "Epoch 47/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0888 - accuracy: 0.9769\n",
            "Epoch 48/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0862 - accuracy: 0.9736\n",
            "Epoch 49/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0858 - accuracy: 0.9670\n",
            "Epoch 50/150\n",
            "303/303 [==============================] - 0s 26us/step - loss: 0.0879 - accuracy: 0.9703\n",
            "Epoch 51/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0836 - accuracy: 0.9802\n",
            "Epoch 52/150\n",
            "303/303 [==============================] - 0s 36us/step - loss: 0.0890 - accuracy: 0.9703\n",
            "Epoch 53/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0841 - accuracy: 0.9769\n",
            "Epoch 54/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.0857 - accuracy: 0.9703\n",
            "Epoch 55/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0861 - accuracy: 0.9736\n",
            "Epoch 56/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0894 - accuracy: 0.9769\n",
            "Epoch 57/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0784 - accuracy: 0.9736\n",
            "Epoch 58/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0819 - accuracy: 0.9769\n",
            "Epoch 59/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0784 - accuracy: 0.9736\n",
            "Epoch 60/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0794 - accuracy: 0.9703\n",
            "Epoch 61/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0805 - accuracy: 0.9736\n",
            "Epoch 62/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0803 - accuracy: 0.9769\n",
            "Epoch 63/150\n",
            "303/303 [==============================] - 0s 37us/step - loss: 0.0817 - accuracy: 0.9736\n",
            "Epoch 64/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0837 - accuracy: 0.9769\n",
            "Epoch 65/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0752 - accuracy: 0.9736\n",
            "Epoch 66/150\n",
            "303/303 [==============================] - 0s 41us/step - loss: 0.0773 - accuracy: 0.9769\n",
            "Epoch 67/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0748 - accuracy: 0.9802\n",
            "Epoch 68/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.0780 - accuracy: 0.9769\n",
            "Epoch 69/150\n",
            "303/303 [==============================] - 0s 26us/step - loss: 0.0777 - accuracy: 0.9703\n",
            "Epoch 70/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0764 - accuracy: 0.9769\n",
            "Epoch 71/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0744 - accuracy: 0.9802\n",
            "Epoch 72/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0676 - accuracy: 0.9769\n",
            "Epoch 73/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0667 - accuracy: 0.9769\n",
            "Epoch 74/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0753 - accuracy: 0.9769\n",
            "Epoch 75/150\n",
            "303/303 [==============================] - 0s 37us/step - loss: 0.0711 - accuracy: 0.9769\n",
            "Epoch 76/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0667 - accuracy: 0.9769\n",
            "Epoch 77/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0693 - accuracy: 0.9802\n",
            "Epoch 78/150\n",
            "303/303 [==============================] - 0s 36us/step - loss: 0.0682 - accuracy: 0.9835\n",
            "Epoch 79/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.0642 - accuracy: 0.9835\n",
            "Epoch 80/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0774 - accuracy: 0.9802\n",
            "Epoch 81/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0669 - accuracy: 0.9835\n",
            "Epoch 82/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0733 - accuracy: 0.9769\n",
            "Epoch 83/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0733 - accuracy: 0.9835\n",
            "Epoch 84/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0700 - accuracy: 0.9769\n",
            "Epoch 85/150\n",
            "303/303 [==============================] - 0s 44us/step - loss: 0.0690 - accuracy: 0.9835\n",
            "Epoch 86/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0647 - accuracy: 0.9802\n",
            "Epoch 87/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.0719 - accuracy: 0.9835\n",
            "Epoch 88/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0684 - accuracy: 0.9868\n",
            "Epoch 89/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0690 - accuracy: 0.9835\n",
            "Epoch 90/150\n",
            "303/303 [==============================] - 0s 40us/step - loss: 0.0704 - accuracy: 0.9835\n",
            "Epoch 91/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0698 - accuracy: 0.9835\n",
            "Epoch 92/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0638 - accuracy: 0.9835\n",
            "Epoch 93/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0665 - accuracy: 0.9868\n",
            "Epoch 94/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0638 - accuracy: 0.9868\n",
            "Epoch 95/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0702 - accuracy: 0.9835\n",
            "Epoch 96/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0686 - accuracy: 0.9868\n",
            "Epoch 97/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0644 - accuracy: 0.9868\n",
            "Epoch 98/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0574 - accuracy: 0.9868\n",
            "Epoch 99/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0660 - accuracy: 0.9835\n",
            "Epoch 100/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0651 - accuracy: 0.9868\n",
            "Epoch 101/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0654 - accuracy: 0.9769\n",
            "Epoch 102/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0638 - accuracy: 0.9802\n",
            "Epoch 103/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0573 - accuracy: 0.9868\n",
            "Epoch 104/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0633 - accuracy: 0.9835\n",
            "Epoch 105/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0607 - accuracy: 0.9835\n",
            "Epoch 106/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0620 - accuracy: 0.9868\n",
            "Epoch 107/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0670 - accuracy: 0.9802\n",
            "Epoch 108/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0584 - accuracy: 0.9835\n",
            "Epoch 109/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0610 - accuracy: 0.9835\n",
            "Epoch 110/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0671 - accuracy: 0.9868\n",
            "Epoch 111/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0635 - accuracy: 0.9835\n",
            "Epoch 112/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.0593 - accuracy: 0.9835\n",
            "Epoch 113/150\n",
            "303/303 [==============================] - 0s 39us/step - loss: 0.0646 - accuracy: 0.9835\n",
            "Epoch 114/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0648 - accuracy: 0.9835\n",
            "Epoch 115/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0652 - accuracy: 0.9802\n",
            "Epoch 116/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0613 - accuracy: 0.9835\n",
            "Epoch 117/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0643 - accuracy: 0.9868\n",
            "Epoch 118/150\n",
            "303/303 [==============================] - 0s 39us/step - loss: 0.0644 - accuracy: 0.9868\n",
            "Epoch 119/150\n",
            "303/303 [==============================] - 0s 40us/step - loss: 0.0608 - accuracy: 0.9802\n",
            "Epoch 120/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0629 - accuracy: 0.9835\n",
            "Epoch 121/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0642 - accuracy: 0.9835\n",
            "Epoch 122/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0673 - accuracy: 0.9835\n",
            "Epoch 123/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0629 - accuracy: 0.9868\n",
            "Epoch 124/150\n",
            "303/303 [==============================] - 0s 46us/step - loss: 0.0625 - accuracy: 0.9868\n",
            "Epoch 125/150\n",
            "303/303 [==============================] - 0s 29us/step - loss: 0.0665 - accuracy: 0.9835\n",
            "Epoch 126/150\n",
            "303/303 [==============================] - 0s 26us/step - loss: 0.0697 - accuracy: 0.9835\n",
            "Epoch 127/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0655 - accuracy: 0.9802\n",
            "Epoch 128/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0609 - accuracy: 0.9868\n",
            "Epoch 129/150\n",
            "303/303 [==============================] - 0s 27us/step - loss: 0.0561 - accuracy: 0.9835\n",
            "Epoch 130/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0661 - accuracy: 0.9802\n",
            "Epoch 131/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0641 - accuracy: 0.9802\n",
            "Epoch 132/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0536 - accuracy: 0.9868\n",
            "Epoch 133/150\n",
            "303/303 [==============================] - 0s 26us/step - loss: 0.0619 - accuracy: 0.9868\n",
            "Epoch 134/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0547 - accuracy: 0.9835\n",
            "Epoch 135/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0621 - accuracy: 0.9868\n",
            "Epoch 136/150\n",
            "303/303 [==============================] - 0s 31us/step - loss: 0.0553 - accuracy: 0.9835\n",
            "Epoch 137/150\n",
            "303/303 [==============================] - 0s 34us/step - loss: 0.0481 - accuracy: 0.9868\n",
            "Epoch 138/150\n",
            "303/303 [==============================] - 0s 41us/step - loss: 0.0625 - accuracy: 0.9835\n",
            "Epoch 139/150\n",
            "303/303 [==============================] - 0s 41us/step - loss: 0.0578 - accuracy: 0.9868\n",
            "Epoch 140/150\n",
            "303/303 [==============================] - 0s 33us/step - loss: 0.0509 - accuracy: 0.9835\n",
            "Epoch 141/150\n",
            "303/303 [==============================] - 0s 35us/step - loss: 0.0583 - accuracy: 0.9868\n",
            "Epoch 142/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0548 - accuracy: 0.9868\n",
            "Epoch 143/150\n",
            "303/303 [==============================] - 0s 37us/step - loss: 0.0523 - accuracy: 0.9868\n",
            "Epoch 144/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0557 - accuracy: 0.9868\n",
            "Epoch 145/150\n",
            "303/303 [==============================] - 0s 28us/step - loss: 0.0555 - accuracy: 0.9868\n",
            "Epoch 146/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0518 - accuracy: 0.9868\n",
            "Epoch 147/150\n",
            "303/303 [==============================] - 0s 38us/step - loss: 0.0531 - accuracy: 0.9868\n",
            "Epoch 148/150\n",
            "303/303 [==============================] - 0s 46us/step - loss: 0.0524 - accuracy: 0.9835\n",
            "Epoch 149/150\n",
            "303/303 [==============================] - 0s 30us/step - loss: 0.0518 - accuracy: 0.9835\n",
            "Epoch 150/150\n",
            "303/303 [==============================] - 0s 32us/step - loss: 0.0580 - accuracy: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f399ddddcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml3tajdTLGgW",
        "colab_type": "code",
        "outputId": "eb4e0f44-ec41-427e-bf58-29d0028e75d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_Fpred = classifierF.predict(X_test)\n",
        "y_Fpred = (y_Fpred > 0.5)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(y_Fpred))*100\n",
        "cm = confusion_matrix(y_test, np.round(y_Fpred))\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX')\n",
        "print(cm)\n",
        "\n",
        "print('\\nTEST METRICS')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX\n",
            "[[43  0]\n",
            " [ 1 32]]\n",
            "\n",
            "TEST METRICS\n",
            "Accuracy: 98.68421052631578%\n",
            "Precision: 100.0%\n",
            "Recall: 96.96969696969697%\n",
            "F1-score: 98.46153846153845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxB5D6Z1LGet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_vU92TLGaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGzecguWLGX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTAV1ISfLGU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}